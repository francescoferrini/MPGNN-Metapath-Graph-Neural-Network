{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5fe0e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, NeighborSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7654bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return print('Error')\n",
    "\n",
    "def one_hot_encoding(l):\n",
    "    label_types = torch.unique(l).tolist()\n",
    "    new_labels = []\n",
    "    for i in range(0, len(l)):\n",
    "        tmp = []\n",
    "        for j in range(0, len(label_types)):\n",
    "            tmp.append(0.)\n",
    "        tmp[l[i].item()] = 1.\n",
    "        new_labels.append(tmp)\n",
    "    return torch.tensor(new_labels)     \n",
    "\n",
    "def load_files(node_file_path, links_file_path, label_file_path, embedding_file_path, dataset):\n",
    "    colors = pd.read_csv(node_file_path, sep='\\t', header = None)\n",
    "    colors = colors.dropna(axis=1,how='all')\n",
    "    labels = pd.read_csv(label_file_path, sep='\\t', header = None)\n",
    "    links = pd.read_csv(links_file_path, sep='\\t', header = None)\n",
    "    labels.rename(columns = {0: 'node', 1: 'label'}, inplace = True)\n",
    "    source_nodes_with_labels = labels['node'].values.tolist()\n",
    "    labels = torch.tensor(labels['label'].values)\n",
    "    colors.rename(columns = {0: 'node', 1: 'color'}, inplace = True)\n",
    "    links.rename(columns = {0: 'node_1', 1: 'relation_type', 2: 'node_2'}, inplace = True)\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        embedding = pd.read_csv(embedding_file_path, sep='\\t', header = None)\n",
    "        embedding_number = len(embedding.columns)-2\n",
    "        if embedding_number == 3:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'second embedding', 2: 'first embedding', 3: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 4:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'third embedding', 2: 'second embedding', 3: 'first embedding', 4: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 5:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'fourth embedding', 2: 'third embedding', 3: 'second embedding', 4: 'first_embdding', 5: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 2:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'first embedding', 2: 'labels'}, inplace = True)\n",
    "        return labels, colors, links, embedding\n",
    "    else:\n",
    "        labels_multi  = one_hot_encoding(labels)\n",
    "        # for i in range(0, len(labels)):\n",
    "        #     if labels[i].item() == 0:\n",
    "        #         labels[i] = 1\n",
    "        #     else:\n",
    "        #         labels[i] = 0\n",
    "        return labels, colors, links, source_nodes_with_labels, labels_multi\n",
    "\n",
    "\n",
    "def get_node_features(colors):\n",
    "    node_features = pd.get_dummies(colors)\n",
    "    \n",
    "    node_features.drop([\"node\"], axis=1, inplace=True)\n",
    "    \n",
    "    x = node_features.to_numpy().astype(np.float32)\n",
    "    x = np.flip(x, 1).copy()\n",
    "    x = torch.from_numpy(x) \n",
    "    return x\n",
    "\n",
    "def get_edge_index_and_type_no_reverse(links):\n",
    "    edge_index = links.drop(['relation_type'], axis=1)\n",
    "    edge_index = torch.tensor([list(edge_index['node_1'].values), list(edge_index['node_2'].values)])\n",
    "    \n",
    "    edge_type = links['relation_type']\n",
    "    edge_type = torch.tensor(edge_type)\n",
    "    return edge_index, edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cfb693e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mpgnn_train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    weight_loss = torch.tensor([1., 100.])\n",
    "    out = model(data.x, data.edge_index, data.edge_type)\n",
    "    loss = F.nll_loss(out[data.train_idx].squeeze(-1), data.train_y)#, weight = weight_loss)\n",
    "    #loss = F.cross_entropy(out[data.train_idx], data.train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def mpgnn_test(model, data):\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_type)#.argmax(dim=-1)\n",
    "    loss = F.nll_loss(pred[data.test_y].squeeze(-1), data.test_y)#, weight = weight_loss)\n",
    "    \n",
    "    train_predictions = torch.argmax(pred[data.train_idx], 1).tolist()\n",
    "    test_predictions = torch.argmax(pred[data.test_idx], 1).tolist()\n",
    "    train_y = data.train_y.tolist()\n",
    "    test_y = data.test_y.tolist()\n",
    "    \n",
    "    f1_train = f1_score(train_predictions, train_y, average='micro')\n",
    "    f1_test_macro = f1_score(test_predictions, test_y, average = 'macro')\n",
    "    f1_test_micro = f1_score(test_predictions, test_y, average = 'micro')\n",
    "    return f1_train, f1_test_micro, f1_test_macro, loss\n",
    "\n",
    "def mpgnn_parallel_multiple(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, metapaths):\n",
    "    #metapaths = [[2, 0]]#, [3, 1]]\n",
    "    #metapaths = [[1, 4, 2, 0], [1, 0], [1, 5, 3, 0]]\n",
    "    #metapaths = [[4, 3, 0], [1, 0], [0, 4, 2]]\n",
    "    #metapaths = [[2, 4, 0], [0, 3, 4], [0, 1]]\n",
    "    \n",
    "    #metapaths = [[2,0],[3,1]] #IMDB\n",
    "    # metapaths = [[0,2],[1,3]] #IMDB\n",
    "    # metapaths = [[0,3],[1,2]]\n",
    "    \n",
    "    #metapaths = [[4, 3, 0], [1, 0], [0, 4, 2]] # syntetic multi\n",
    "    \n",
    "    metapaths = [[2,0],[3,1]]\n",
    "    #metapaths = [[0,0],[0,1],[0,2],[0,3],[1,0],[1,1],[1,2],[1,3],[2,0],[2,1],[2,2],[2,3],[3,0],[3,1],[3,2],[3,3]]\n",
    "    \n",
    "    mpgnn_model = MPNetm(input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, len(metapaths), metapaths)\n",
    "    print(mpgnn_model)\n",
    "    mpgnn_optimizer = torch.optim.Adam(mpgnn_model.parameters(), lr=0.01, weight_decay=0.0005) # lr = 0.01\n",
    "    best_macro, best_micro = 0., 0.\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        loss = mpgnn_train(mpgnn_model, mpgnn_optimizer, data_mpgnn)\n",
    "        \n",
    "        if epoch == 1:\n",
    "            train_acc, f1_test_micro, f1_test_macro,loss_test = mpgnn_test(mpgnn_model, data_mpgnn)\n",
    "            print(epoch, \"Train %.4f\" % loss,\n",
    "                   \"\\tTest %.4f\" % loss_test,\n",
    "                   \"\\tF1 Train %.4f\" % train_acc,\n",
    "                   \"\\tF1 Train %.4f\" % f1_test_micro)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            train_acc, f1_test_micro, f1_test_macro,loss_test = mpgnn_test(mpgnn_model, data_mpgnn)\n",
    "            \n",
    "            print(epoch, \"Train %.4f\" % loss,\n",
    "                   \"\\tTest %.4f\" % loss_test,\n",
    "                   \"\\tF1 Train %.4f\" % train_acc,\n",
    "                   \"\\tF1 Train %.4f\" % f1_test_micro)\n",
    "            \n",
    "            if f1_test_macro > best_macro:\n",
    "                best_macro = f1_test_micro\n",
    "            if f1_test_micro > best_micro:\n",
    "                best_micro = f1_test_micro\n",
    "                \n",
    "    return best_micro, mpgnn_model, mpgnn_optimizer, data_mpgnn\n",
    "\n",
    "def main(node_file_path, link_file_path, label_file_path, embedding_file_path, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset):\n",
    "    # Obtain true 0|1 labels for each node, feature matrix (1-hot encoding) and links among nodes\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        sources = []\n",
    "        true_labels, features, edges, embedding = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "    else: \n",
    "        true_labels, features, edges, sources, labels_multi = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "    # Get features' matrix\n",
    "    x = get_node_features(features)\n",
    "    # Get edge_index and types\n",
    "    edge_index, edge_type = get_edge_index_and_type_no_reverse(edges)\n",
    "\n",
    "    # Split data into train and test\n",
    "    node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(true_labels, features, sources, dataset)\n",
    "    #node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(labels_multi, features, sources, dataset)\n",
    "\n",
    "    # Dataset for MPGNN\n",
    "    data_mpgnn = Data()\n",
    "    data_mpgnn.x = x\n",
    "    data_mpgnn.edge_index = edge_index\n",
    "    data_mpgnn.edge_type = edge_type\n",
    "    data_mpgnn.train_idx = train_idx\n",
    "    data_mpgnn.test_idx = test_idx\n",
    "    data_mpgnn.train_y = train_y\n",
    "    data_mpgnn.test_y = test_y\n",
    "    data_mpgnn.num_nodes = node_idx.size(0)\n",
    "    # Variables\n",
    "    if sources:\n",
    "        source_nodes_mask = sources\n",
    "    else:\n",
    "        source_nodes_mask = []\n",
    "    metapath = []\n",
    "\n",
    "    # Dataset for score function\n",
    "    data = Data()\n",
    "    data.x = x\n",
    "    data.edge_index = edge_index\n",
    "    data.edge_type = edge_type\n",
    "    data.labels = true_labels\n",
    "    data.labels = data.labels.unsqueeze(-1)\n",
    "    data.num_nodes = x.size(0)\n",
    "    data.bags = torch.empty(1)\n",
    "    data.bag_labels = torch.empty(1)\n",
    "\n",
    "    # All possible relations\n",
    "    relations = torch.unique(data.edge_type).tolist()\n",
    "    mp = []\n",
    "      \n",
    "    mpgnn_f1_micro, mod, opt, dat = mpgnn_parallel_multiple(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, mp)\n",
    "    print(mpgnn_f1_micro)\n",
    "    \n",
    "    return mod,opt,dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7b0f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "\n",
    "def splitting_node_and_labels(lab, feat, src, dataset):\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        node_idx = list(feat['node'].values)\n",
    "    else:\n",
    "        node_idx = src.copy()\n",
    "\n",
    "    train_idx,test_idx,train_y,test_y = train_test_split(node_idx, lab,\n",
    "                                                            random_state=111,#415,\n",
    "                                                            stratify=lab, \n",
    "                                                            test_size=0.2)\n",
    "    \n",
    "    return torch.tensor(node_idx), train_idx, train_y, test_idx, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18760780",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLEX = True\n",
    "#COMPLEX = \"synthetic_multi\"\n",
    "COMPLEX = \"IMDB\"\n",
    "\n",
    "metapath_length= 3\n",
    "tot_rel=5\n",
    "\n",
    "if COMPLEX == True:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"complex\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == False:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"simple\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == 'IMDB':\n",
    "    tot_rel=4\n",
    "    input_dim = 3066\n",
    "    ll_output_dim = 3\n",
    "    dataset = 'IMDB' ## 5\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'DBLP':\n",
    "    input_dim = 4231\n",
    "    tot_rel=6\n",
    "    ll_output_dim = 4\n",
    "    dataset = 'DBLP' ## 7\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'synthetic_multi':\n",
    "    input_dim=6\n",
    "    tot_rel=5\n",
    "    ll_output_dim=2\n",
    "    dataset = 'tot_rel_5'\n",
    "    folder=\"data/synthetic_multi/\" + dataset + \"/\"\n",
    "\n",
    "node_file= folder + \"node.dat\"\n",
    "link_file= folder + \"link.dat\"\n",
    "label_file= folder + \"label.dat\"\n",
    "embedding_file = folder + \"embedding.dat\"\n",
    "# Define the filename for saving the variables\n",
    "pickle_filename = folder + \"iteration_variables.pkl\"\n",
    "# mpgnn variables\n",
    "hidden_dim = 16\n",
    "num_rel = tot_rel\n",
    "output_dim = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6ae64f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetm(\n",
      "  (layers_list): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): CustomRGCNConv(3066, 16, num_relations=4)\n",
      "      (1): CustomRGCNConv(16, 16, num_relations=4)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): CustomRGCNConv(3066, 16, num_relations=4)\n",
      "      (1): CustomRGCNConv(16, 16, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.9, inplace=False)\n",
      "  (norm1): LayerNorm(32, mode=graph)\n",
      "  (fc1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (norm2): LayerNorm(16, mode=graph)\n",
      "  (fc2): Linear(in_features=16, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "1 Train 1.1119 \tTest 1.0404 \tF1 Train 0.3758 \tF1 Train 0.3727\n",
      "10 Train 1.0261 \tTest 1.0295 \tF1 Train 0.6075 \tF1 Train 0.5479\n",
      "20 Train 0.8831 \tTest 1.1775 \tF1 Train 0.7975 \tF1 Train 0.6414\n",
      "30 Train 0.7691 \tTest 1.9942 \tF1 Train 0.8366 \tF1 Train 0.6636\n",
      "40 Train 0.7298 \tTest 2.0415 \tF1 Train 0.8518 \tF1 Train 0.6343\n",
      "50 Train 0.6909 \tTest 2.3674 \tF1 Train 0.8892 \tF1 Train 0.6752\n",
      "60 Train 0.6865 \tTest 2.4286 \tF1 Train 0.8901 \tF1 Train 0.6624\n",
      "70 Train 0.6795 \tTest 2.4985 \tF1 Train 0.9077 \tF1 Train 0.6741\n",
      "80 Train 0.6466 \tTest 2.3396 \tF1 Train 0.9132 \tF1 Train 0.6752\n",
      "90 Train 0.6456 \tTest 2.6755 \tF1 Train 0.9155 \tF1 Train 0.6857\n",
      "100 Train 0.6411 \tTest 2.3332 \tF1 Train 0.9229 \tF1 Train 0.6916\n",
      "0.6915887850467289\n"
     ]
    }
   ],
   "source": [
    "mod,opt,dat = main(node_file, link_file, label_file, embedding_file, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964df7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a8ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313e5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57391987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "out = mod(dat.x, dat.edge_index, dat.edge_type)\n",
    "out = torch.argmax(out,1)\n",
    "\n",
    "print(\"train \\t\",np.unique(out[dat.train_idx].tolist(),return_counts=True),\"\\treal \\t\",np.unique(dat.train_y.tolist(),return_counts=True)[1])\n",
    "print(\"test  \\t\",np.unique(out[dat.test_idx].tolist(),return_counts=True),\"\\treal \\t\",np.unique(dat.test_y.tolist(),return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mod(dat.x, dat.edge_index, dat.edge_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "734f13ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[11616, 3066], edge_index=[2, 34212], edge_type=[34212], train_idx=[3422], test_idx=[856], train_y=[3422], test_y=[856], num_nodes=4278)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8dc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea129aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b9dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8b2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073ef57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378de45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b781643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fc0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da47d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de23143",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLEX = True\n",
    "COMPLEX = \"synthetic_multi\"\n",
    "COMPLEX = \"IMDB\"\n",
    "\n",
    "metapath_length= 3\n",
    "tot_rel=5\n",
    "\n",
    "if COMPLEX == True:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"complex\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == False:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"simple\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == 'IMDB':\n",
    "    tot_rel=4\n",
    "    input_dim = 3066\n",
    "    ll_output_dim = 3\n",
    "    dataset = 'IMDB' ## 5\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'DBLP':\n",
    "    input_dim = 4231\n",
    "    tot_rel=6\n",
    "    ll_output_dim = 4\n",
    "    dataset = 'DBLP' ## 7\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'synthetic_multi':\n",
    "    input_dim=6\n",
    "    tot_rel=5\n",
    "    ll_output_dim=2\n",
    "    dataset = 'tot_rel_5'\n",
    "    folder=\"data/synthetic_multi/\" + dataset + \"/\"\n",
    "\n",
    "node_file= folder + \"node.dat\"\n",
    "link_file= folder + \"link.dat\"\n",
    "label_file= folder + \"label.dat\"\n",
    "embedding_file = folder + \"embedding.dat\"\n",
    "# Define the filename for saving the variables\n",
    "pickle_filename = folder + \"iteration_variables.pkl\"\n",
    "# mpgnn variables\n",
    "hidden_dim = 32\n",
    "num_rel = tot_rel\n",
    "output_dim = 64\n",
    "\n",
    "node_file_path, link_file_path, label_file_path, embedding_file_path, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset = node_file, link_file, label_file, embedding_file, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b55b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtain true 0|1 labels for each node, feature matrix (1-hot encoding) and links among nodes\n",
    "if dataset == 'complex' or dataset == 'simple':\n",
    "    sources = []\n",
    "    true_labels, features, edges, embedding = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "else: \n",
    "    true_labels, features, edges, sources, labels_multi = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "# Get features' matrix\n",
    "x = get_node_features(features)\n",
    "# Get edge_index and types\n",
    "edge_index, edge_type = get_edge_index_and_type_no_reverse(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4870ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47463a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79df6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33374d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df96147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae2bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bbba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0c8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e1835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49e518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580be425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e57a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61829c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
