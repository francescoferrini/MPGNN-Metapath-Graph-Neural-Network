{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fe0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, NeighborSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7654bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return print('Error')\n",
    "\n",
    "def one_hot_encoding(l):\n",
    "    label_types = torch.unique(l).tolist()\n",
    "    new_labels = []\n",
    "    for i in range(0, len(l)):\n",
    "        tmp = []\n",
    "        for j in range(0, len(label_types)):\n",
    "            tmp.append(0.)\n",
    "        tmp[l[i].item()] = 1.\n",
    "        new_labels.append(tmp)\n",
    "    return torch.tensor(new_labels)     \n",
    "\n",
    "def load_files(node_file_path, links_file_path, label_file_path, embedding_file_path, dataset):\n",
    "    colors = pd.read_csv(node_file_path, sep='\\t', header = None)\n",
    "    colors = colors.dropna(axis=1,how='all')\n",
    "    labels = pd.read_csv(label_file_path, sep='\\t', header = None)\n",
    "    links = pd.read_csv(links_file_path, sep='\\t', header = None)\n",
    "    labels.rename(columns = {0: 'node', 1: 'label'}, inplace = True)\n",
    "    source_nodes_with_labels = labels['node'].values.tolist()\n",
    "    labels = torch.tensor(labels['label'].values)\n",
    "    colors.rename(columns = {0: 'node', 1: 'color'}, inplace = True)\n",
    "    links.rename(columns = {0: 'node_1', 1: 'relation_type', 2: 'node_2'}, inplace = True)\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        embedding = pd.read_csv(embedding_file_path, sep='\\t', header = None)\n",
    "        embedding_number = len(embedding.columns)-2\n",
    "        if embedding_number == 3:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'second embedding', 2: 'first embedding', 3: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 4:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'third embedding', 2: 'second embedding', 3: 'first embedding', 4: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 5:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'fourth embedding', 2: 'third embedding', 3: 'second embedding', 4: 'first_embdding', 5: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 2:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'first embedding', 2: 'labels'}, inplace = True)\n",
    "        return labels, colors, links, embedding\n",
    "    else:\n",
    "        labels_multi  = one_hot_encoding(labels)\n",
    "        # for i in range(0, len(labels)):\n",
    "        #     if labels[i].item() == 0:\n",
    "        #         labels[i] = 1\n",
    "        #     else:\n",
    "        #         labels[i] = 0\n",
    "        return labels, colors, links, source_nodes_with_labels, labels_multi\n",
    "\n",
    "def splitting_node_and_labels(lab, feat, src, dataset):\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        node_idx = torch.tensor(feat['node'].values)\n",
    "    else:\n",
    "        node_idx = torch.tensor(src)\n",
    "    train_split = int(len(node_idx)*0.8)\n",
    "    test_split = len(node_idx) - train_split\n",
    "    train_idx = node_idx[:train_split]\n",
    "    test_idx = node_idx[-test_split:]\n",
    "\n",
    "    train_y = lab[:train_split]\n",
    "    test_y = lab[-test_split:]\n",
    "    return node_idx, train_idx, train_y, test_idx, test_y\n",
    "\n",
    "def get_node_features(colors):\n",
    "    node_features = pd.get_dummies(colors)\n",
    "    \n",
    "    node_features.drop([\"node\"], axis=1, inplace=True)\n",
    "    \n",
    "    x = node_features.to_numpy().astype(np.float32)\n",
    "    x = np.flip(x, 1).copy()\n",
    "    x = torch.from_numpy(x) \n",
    "    return x\n",
    "\n",
    "def get_edge_index_and_type_no_reverse(links):\n",
    "    edge_index = links.drop(['relation_type'], axis=1)\n",
    "    edge_index = torch.tensor([list(edge_index['node_1'].values), list(edge_index['node_2'].values)])\n",
    "    \n",
    "    edge_type = links['relation_type']\n",
    "    edge_type = torch.tensor(edge_type)\n",
    "    return edge_index, edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfb693e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mpgnn_train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    weight_loss = torch.tensor([1., 100.])\n",
    "    out = model(data.x, data.edge_index, data.edge_type)\n",
    "    loss = F.nll_loss(out[data.train_idx].squeeze(-1), data.train_y)#, weight = weight_loss)\n",
    "    #loss = F.cross_entropy(out[data.train_idx], data.train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def mpgnn_test(model, data):\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_type)#.argmax(dim=-1)\n",
    "    train_predictions = torch.argmax(pred[data.train_idx], 1).tolist()\n",
    "    test_predictions = torch.argmax(pred[data.test_idx], 1).tolist()\n",
    "    train_y = data.train_y.tolist()\n",
    "    test_y = data.test_y.tolist()\n",
    "    # train_acc = (train_predictions == train_y).float().mean()\n",
    "    # test_acc = (test_predictions == test_y).float().mean()\n",
    "    f1_train = f1_score(train_predictions, train_y, average='macro')\n",
    "    f1_test_macro = f1_score(test_predictions, test_y, average = 'macro')\n",
    "    f1_test_micro = f1_score(test_predictions, test_y, average = 'micro')\n",
    "    return f1_train, f1_test_micro, f1_test_macro\n",
    "\n",
    "def mpgnn_parallel(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, metapath):\n",
    "    metapath=[0, 1, 2]\n",
    "    mpgnn_model = MPNet(input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, len(metapath), metapath)\n",
    "    print(mpgnn_model)\n",
    "    # for name, param in mpgnn_model.named_parameters():\n",
    "    #     print(name, param, param.size())\n",
    "    mpgnn_optimizer = torch.optim.Adam(mpgnn_model.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "    best_macro, best_micro = 0., 0.\n",
    "    for epoch in tqdm(range(1, 100)):\n",
    "        loss = mpgnn_train(mpgnn_model, mpgnn_optimizer, data_mpgnn)\n",
    "        train_acc, f1_test_micro, f1_test_macro = mpgnn_test(mpgnn_model, data_mpgnn)\n",
    "        if f1_test_macro > best_macro:\n",
    "            best_macro = f1_test_micro\n",
    "        if f1_test_micro > best_micro:\n",
    "            best_micro = f1_test_micro\n",
    "    return best_micro\n",
    "\n",
    "def mpgnn_parallel_multiple(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, metapaths):\n",
    "    #metapaths = [[2, 0]]#, [3, 1]]\n",
    "    #metapaths = [[1, 4, 2, 0], [1, 0], [1, 5, 3, 0]]\n",
    "    #metapaths = [[4, 3, 0], [1, 0], [0, 4, 2]]\n",
    "    #metapaths = [[2, 4, 0], [0, 3, 4], [0, 1]]\n",
    "    metapaths = [[2,0],[3,1]] #IMDB\n",
    "    metapaths = [[0,2],[1,3]] #IMDB\n",
    "    \n",
    "    mpgnn_model = MPNetm(input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, len(metapaths), metapaths)\n",
    "    print(mpgnn_model)\n",
    "    # for name, param in mpgnn_model.named_parameters():\n",
    "    #     print(name, param, param.size())\n",
    "    mpgnn_optimizer = torch.optim.Adam(mpgnn_model.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "    best_macro, best_micro = 0., 0.\n",
    "    for epoch in range(1, 1000):\n",
    "        loss = mpgnn_train(mpgnn_model, mpgnn_optimizer, data_mpgnn)\n",
    "        train_acc, f1_test_micro, f1_test_macro = mpgnn_test(mpgnn_model, data_mpgnn)\n",
    "        print(epoch, 'loss: ', loss, 'train acc: ', train_acc, 'micro: ', f1_test_micro)\n",
    "        if f1_test_macro > best_macro:\n",
    "            best_macro = f1_test_micro\n",
    "        if f1_test_micro > best_micro:\n",
    "            best_micro = f1_test_micro\n",
    "    return best_micro\n",
    "\n",
    "def main(node_file_path, link_file_path, label_file_path, embedding_file_path, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset):\n",
    "    # Obtain true 0|1 labels for each node, feature matrix (1-hot encoding) and links among nodes\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        sources = []\n",
    "        true_labels, features, edges, embedding = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "    else: \n",
    "        true_labels, features, edges, sources, labels_multi = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "    # Get features' matrix\n",
    "    x = get_node_features(features)\n",
    "    # Get edge_index and types\n",
    "    edge_index, edge_type = get_edge_index_and_type_no_reverse(edges)\n",
    "\n",
    "    # Split data into train and test\n",
    "    node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(true_labels, features, sources, dataset)\n",
    "    #node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(labels_multi, features, sources, dataset)\n",
    "\n",
    "    # Dataset for MPGNN\n",
    "    data_mpgnn = Data()\n",
    "    data_mpgnn.x = x\n",
    "    data_mpgnn.edge_index = edge_index\n",
    "    data_mpgnn.edge_type = edge_type\n",
    "    data_mpgnn.train_idx = train_idx\n",
    "    data_mpgnn.test_idx = test_idx\n",
    "    data_mpgnn.train_y = train_y\n",
    "    data_mpgnn.test_y = test_y\n",
    "    data_mpgnn.num_nodes = node_idx.size(0)\n",
    "    # Variables\n",
    "    if sources:\n",
    "        source_nodes_mask = sources\n",
    "    else:\n",
    "        source_nodes_mask = []\n",
    "    metapath = []\n",
    "\n",
    "    # Dataset for score function\n",
    "    data = Data()\n",
    "    data.x = x\n",
    "    data.edge_index = edge_index\n",
    "    data.edge_type = edge_type\n",
    "    data.labels = true_labels\n",
    "    data.labels = data.labels.unsqueeze(-1)\n",
    "    data.num_nodes = x.size(0)\n",
    "    data.bags = torch.empty(1)\n",
    "    data.bag_labels = torch.empty(1)\n",
    "\n",
    "    # All possible relations\n",
    "    relations = torch.unique(data.edge_type).tolist()\n",
    "    mp = []\n",
    "    mpgnn_f1_micro = mpgnn_parallel_multiple(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, mp)\n",
    "    print(mpgnn_f1_micro)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18760780",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLEX = True\n",
    "COMPLEX = \"synthetic_multi\"\n",
    "COMPLEX = \"IMDB\"\n",
    "\n",
    "metapath_length= 3\n",
    "tot_rel=5\n",
    "\n",
    "if COMPLEX == True:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"complex\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == False:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"simple\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == 'IMDB':\n",
    "    tot_rel=4\n",
    "    input_dim = 3066\n",
    "    ll_output_dim = 3\n",
    "    dataset = 'IMDB' ## 5\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'DBLP':\n",
    "    input_dim = 4231\n",
    "    tot_rel=6\n",
    "    ll_output_dim = 4\n",
    "    dataset = 'DBLP' ## 7\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'synthetic_multi':\n",
    "    input_dim=6\n",
    "    tot_rel=5\n",
    "    ll_output_dim=2\n",
    "    dataset = 'tot_rel_5'\n",
    "    folder=\"data/synthetic_multi/\" + dataset + \"/\"\n",
    "\n",
    "node_file= folder + \"node.dat\"\n",
    "link_file= folder + \"link.dat\"\n",
    "label_file= folder + \"label.dat\"\n",
    "embedding_file = folder + \"embedding.dat\"\n",
    "# Define the filename for saving the variables\n",
    "pickle_filename = folder + \"iteration_variables.pkl\"\n",
    "# mpgnn variables\n",
    "hidden_dim = 32\n",
    "num_rel = tot_rel\n",
    "output_dim = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6ae64f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetm(\n",
      "  (layers_list): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): CustomRGCNConv(3066, 32, num_relations=4)\n",
      "      (1): CustomRGCNConv(32, 32, num_relations=4)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): CustomRGCNConv(3066, 32, num_relations=4)\n",
      "      (1): CustomRGCNConv(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "1 loss:  1.0967216491699219 train acc:  0.17719647804962432 micro:  0.4100467289719626\n",
      "2 loss:  1.0876092910766602 train acc:  0.40422099510786486 micro:  0.4672897196261683\n",
      "3 loss:  1.065207839012146 train acc:  0.48554254958078014 micro:  0.5397196261682243\n",
      "4 loss:  1.0188186168670654 train acc:  0.4999179517148802 micro:  0.5829439252336449\n",
      "5 loss:  0.9465958476066589 train acc:  0.5304791681743052 micro:  0.5817757009345794\n",
      "6 loss:  0.84529048204422 train acc:  0.669466400790118 micro:  0.6051401869158879\n",
      "7 loss:  0.7170555591583252 train acc:  0.7929543324553299 micro:  0.6121495327102804\n",
      "8 loss:  0.5818178057670593 train acc:  0.8818578466493884 micro:  0.610981308411215\n",
      "9 loss:  0.4473559856414795 train acc:  0.9397282410079644 micro:  0.6039719626168224\n",
      "10 loss:  0.32009661197662354 train acc:  0.9594623436941566 micro:  0.6051401869158879\n",
      "11 loss:  0.21906189620494843 train acc:  0.9661844643056451 micro:  0.580607476635514\n",
      "12 loss:  0.14533644914627075 train acc:  0.9808664439111433 micro:  0.5992990654205608\n",
      "13 loss:  0.08943972736597061 train acc:  0.986168181125897 micro:  0.5852803738317757\n",
      "14 loss:  0.06105164438486099 train acc:  0.9928386132178583 micro:  0.5864485981308412\n",
      "15 loss:  0.041334375739097595 train acc:  0.9946364930867769 micro:  0.5677570093457944\n",
      "16 loss:  0.030833905562758446 train acc:  0.9973109639596721 micro:  0.5642523364485982\n",
      "17 loss:  0.021989155560731888 train acc:  0.9979043732603868 micro:  0.5537383177570093\n",
      "18 loss:  0.018363039940595627 train acc:  0.998506809801745 micro:  0.5572429906542056\n",
      "19 loss:  0.015590847469866276 train acc:  0.998506809801745 micro:  0.5502336448598131\n",
      "20 loss:  0.013904388062655926 train acc:  0.9988036295467682 micro:  0.544392523364486\n",
      "21 loss:  0.012919534929096699 train acc:  0.9988036295467682 micro:  0.5432242990654206\n",
      "22 loss:  0.011496467515826225 train acc:  0.9991004957530065 micro:  0.5537383177570093\n",
      "23 loss:  0.01036109309643507 train acc:  0.9991004957530065 micro:  0.5572429906542056\n",
      "24 loss:  0.0101381316781044 train acc:  0.9991004957530065 micro:  0.5572429906542056\n",
      "25 loss:  0.009064680896699429 train acc:  0.999702882610062 micro:  0.5619158878504673\n",
      "26 loss:  0.00859199557453394 train acc:  0.999702882610062 micro:  0.5677570093457944\n",
      "27 loss:  0.007906906306743622 train acc:  1.0 micro:  0.5689252336448598\n",
      "28 loss:  0.007722537964582443 train acc:  1.0 micro:  0.572429906542056\n",
      "29 loss:  0.00767764775082469 train acc:  1.0 micro:  0.5630841121495327\n",
      "30 loss:  0.007607293780893087 train acc:  1.0 micro:  0.5735981308411215\n",
      "31 loss:  0.007680471520870924 train acc:  1.0 micro:  0.5595794392523364\n",
      "32 loss:  0.00817084964364767 train acc:  1.0 micro:  0.5654205607476636\n",
      "33 loss:  0.009160766378045082 train acc:  1.0 micro:  0.5490654205607477\n",
      "34 loss:  0.008919749408960342 train acc:  1.0 micro:  0.5700934579439252\n",
      "35 loss:  0.007109449245035648 train acc:  1.0 micro:  0.5619158878504673\n",
      "36 loss:  0.004813467618077993 train acc:  1.0 micro:  0.5595794392523364\n",
      "37 loss:  0.005540587939321995 train acc:  1.0 micro:  0.5712616822429907\n",
      "38 loss:  0.004665125627070665 train acc:  1.0 micro:  0.572429906542056\n",
      "39 loss:  0.004550177603960037 train acc:  1.0 micro:  0.5595794392523364\n",
      "40 loss:  0.004314500372856855 train acc:  1.0 micro:  0.5607476635514018\n",
      "41 loss:  0.003952938597649336 train acc:  1.0 micro:  0.5630841121495327\n",
      "42 loss:  0.003855477087199688 train acc:  1.0 micro:  0.5759345794392523\n",
      "43 loss:  0.0037248944863677025 train acc:  1.0 micro:  0.5759345794392523\n",
      "44 loss:  0.003579850075766444 train acc:  1.0 micro:  0.5782710280373832\n",
      "45 loss:  0.0035158214159309864 train acc:  1.0 micro:  0.5759345794392523\n",
      "46 loss:  0.0034638058859854937 train acc:  1.0 micro:  0.580607476635514\n",
      "47 loss:  0.0035486258566379547 train acc:  1.0 micro:  0.5735981308411215\n",
      "48 loss:  0.0035620236303657293 train acc:  1.0 micro:  0.572429906542056\n",
      "49 loss:  0.003616493195295334 train acc:  1.0 micro:  0.5689252336448598\n",
      "50 loss:  0.003661706577986479 train acc:  1.0 micro:  0.5712616822429907\n",
      "51 loss:  0.0036969492211937904 train acc:  1.0 micro:  0.5782710280373832\n",
      "52 loss:  0.0036956819240003824 train acc:  1.0 micro:  0.580607476635514\n",
      "53 loss:  0.0036320306826382875 train acc:  1.0 micro:  0.5841121495327103\n",
      "54 loss:  0.0035309691447764635 train acc:  1.0 micro:  0.5817757009345794\n",
      "55 loss:  0.003422538982704282 train acc:  1.0 micro:  0.5841121495327103\n",
      "56 loss:  0.0033081641886383295 train acc:  1.0 micro:  0.580607476635514\n",
      "57 loss:  0.0031907192897051573 train acc:  1.0 micro:  0.5829439252336449\n",
      "58 loss:  0.0031033665873110294 train acc:  1.0 micro:  0.5829439252336449\n",
      "59 loss:  0.0030767174903303385 train acc:  1.0 micro:  0.5829439252336449\n",
      "60 loss:  0.0030175382271409035 train acc:  1.0 micro:  0.5852803738317757\n",
      "61 loss:  0.002991326851770282 train acc:  1.0 micro:  0.5794392523364486\n",
      "62 loss:  0.0029732808470726013 train acc:  1.0 micro:  0.5782710280373832\n",
      "63 loss:  0.002944841282442212 train acc:  1.0 micro:  0.5782710280373832\n",
      "64 loss:  0.002896550577133894 train acc:  1.0 micro:  0.5747663551401869\n",
      "65 loss:  0.0028779609128832817 train acc:  1.0 micro:  0.5771028037383178\n",
      "66 loss:  0.0028473648708313704 train acc:  1.0 micro:  0.5794392523364486\n",
      "67 loss:  0.0028129657730460167 train acc:  1.0 micro:  0.5841121495327103\n",
      "68 loss:  0.0027837089728564024 train acc:  1.0 micro:  0.5876168224299065\n",
      "69 loss:  0.0027579735033214092 train acc:  1.0 micro:  0.5817757009345794\n",
      "70 loss:  0.002736963564530015 train acc:  1.0 micro:  0.5841121495327103\n",
      "71 loss:  0.002719902666285634 train acc:  1.0 micro:  0.5829439252336449\n",
      "72 loss:  0.0027001332491636276 train acc:  1.0 micro:  0.5794392523364486\n",
      "73 loss:  0.002676967764273286 train acc:  1.0 micro:  0.5794392523364486\n",
      "74 loss:  0.0026512376498430967 train acc:  1.0 micro:  0.5771028037383178\n",
      "75 loss:  0.0026210101787000895 train acc:  1.0 micro:  0.5771028037383178\n",
      "76 loss:  0.0026056268252432346 train acc:  1.0 micro:  0.5747663551401869\n",
      "77 loss:  0.0025963035877794027 train acc:  1.0 micro:  0.5829439252336449\n",
      "78 loss:  0.002612105570733547 train acc:  1.0 micro:  0.572429906542056\n",
      "79 loss:  0.00260493578389287 train acc:  1.0 micro:  0.5782710280373832\n",
      "80 loss:  0.0025983902160078287 train acc:  1.0 micro:  0.5735981308411215\n",
      "81 loss:  0.002542373724281788 train acc:  1.0 micro:  0.5782710280373832\n",
      "82 loss:  0.0024997603613883257 train acc:  1.0 micro:  0.5747663551401869\n",
      "83 loss:  0.002477377885952592 train acc:  1.0 micro:  0.572429906542056\n",
      "84 loss:  0.0024920348078012466 train acc:  1.0 micro:  0.5735981308411215\n",
      "85 loss:  0.0025416812859475613 train acc:  1.0 micro:  0.5712616822429907\n",
      "86 loss:  0.0025803926400840282 train acc:  1.0 micro:  0.5782710280373832\n",
      "87 loss:  0.0026524856220930815 train acc:  1.0 micro:  0.5654205607476636\n",
      "88 loss:  0.0026765945367515087 train acc:  1.0 micro:  0.5841121495327103\n",
      "89 loss:  0.0026510434690862894 train acc:  1.0 micro:  0.5630841121495327\n",
      "90 loss:  0.002426723251119256 train acc:  1.0 micro:  0.5759345794392523\n",
      "91 loss:  0.0022467991802841425 train acc:  1.0 micro:  0.5759345794392523\n",
      "92 loss:  0.0023524172138422728 train acc:  1.0 micro:  0.5630841121495327\n",
      "93 loss:  0.0024234475567936897 train acc:  1.0 micro:  0.5782710280373832\n",
      "94 loss:  0.002372459741309285 train acc:  1.0 micro:  0.572429906542056\n",
      "95 loss:  0.0022990701254457235 train acc:  1.0 micro:  0.5735981308411215\n",
      "96 loss:  0.0023748171515762806 train acc:  1.0 micro:  0.5782710280373832\n",
      "97 loss:  0.002553278114646673 train acc:  1.0 micro:  0.5665887850467289\n",
      "98 loss:  0.002770158462226391 train acc:  1.0 micro:  0.5934579439252337\n",
      "99 loss:  0.003413473255932331 train acc:  1.0 micro:  0.5292056074766355\n",
      "100 loss:  0.007799220271408558 train acc:  0.8004432228389379 micro:  0.5981308411214953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 loss:  0.5213574171066284 train acc:  0.2645664207429261 micro:  0.1658878504672897\n",
      "102 loss:  5.079266548156738 train acc:  0.5426563362390558 micro:  0.45794392523364486\n",
      "103 loss:  3.767960786819458 train acc:  0.5336021171642022 micro:  0.4369158878504673\n",
      "104 loss:  2.9091877937316895 train acc:  0.8645329911190339 micro:  0.5549065420560748\n",
      "105 loss:  0.48006579279899597 train acc:  0.8027086819535193 micro:  0.5619158878504673\n",
      "106 loss:  0.8859720826148987 train acc:  0.790343855388613 micro:  0.5537383177570093\n",
      "107 loss:  0.8613075613975525 train acc:  0.9124661160592696 micro:  0.6039719626168224\n",
      "108 loss:  0.29657870531082153 train acc:  0.9528146804410461 micro:  0.6191588785046729\n",
      "109 loss:  0.1436212807893753 train acc:  0.9507220724292275 micro:  0.602803738317757\n",
      "110 loss:  0.1406417340040207 train acc:  0.9416836029992233 micro:  0.5911214953271028\n",
      "111 loss:  0.17102518677711487 train acc:  0.9370000621336573 micro:  0.5887850467289719\n",
      "112 loss:  0.18828287720680237 train acc:  0.9436572790276824 micro:  0.5817757009345794\n",
      "113 loss:  0.18016240000724792 train acc:  0.9574602063077172 micro:  0.5829439252336449\n",
      "114 loss:  0.15498320758342743 train acc:  0.9700006528985486 micro:  0.5782710280373832\n",
      "115 loss:  0.12599563598632812 train acc:  0.9801501153551976 micro:  0.5841121495327103\n",
      "116 loss:  0.1032186821103096 train acc:  0.9839220943989909 micro:  0.5981308411214953\n",
      "117 loss:  0.08838683366775513 train acc:  0.985599671403215 micro:  0.6016355140186916\n",
      "118 loss:  0.07886585593223572 train acc:  0.9893293975988423 micro:  0.6063084112149533\n",
      "119 loss:  0.07267382740974426 train acc:  0.9914202964062341 micro:  0.6074766355140186\n",
      "120 loss:  0.06811171025037766 train acc:  0.9916989412002756 micro:  0.602803738317757\n",
      "121 loss:  0.06410779803991318 train acc:  0.9931451946910034 micro:  0.5992990654205608\n",
      "122 loss:  0.059742271900177 train acc:  0.9948363362449403 micro:  0.6016355140186916\n",
      "123 loss:  0.05484376475214958 train acc:  0.99571746099704 micro:  0.6004672897196262\n",
      "124 loss:  0.04970281571149826 train acc:  0.9971473362787204 micro:  0.6039719626168224\n",
      "125 loss:  0.04497509449720383 train acc:  0.997705422650375 micro:  0.6039719626168224\n",
      "126 loss:  0.04092903807759285 train acc:  0.9980112363108331 micro:  0.6051401869158879\n",
      "127 loss:  0.03731503710150719 train acc:  0.9985873162208817 micro:  0.6051401869158879\n",
      "128 loss:  0.03400963917374611 train acc:  0.998569198447434 micro:  0.6039719626168224\n",
      "129 loss:  0.031014101579785347 train acc:  0.998569198447434 micro:  0.5992990654205608\n",
      "130 loss:  0.028305023908615112 train acc:  0.998569198447434 micro:  0.5992990654205608\n",
      "131 loss:  0.025844551622867584 train acc:  0.9988215148717087 micro:  0.6016355140186916\n",
      "132 loss:  0.023541873320937157 train acc:  0.9991365454248194 micro:  0.6004672897196262\n",
      "133 loss:  0.021296413615345955 train acc:  0.9991365454248194 micro:  0.5981308411214953\n",
      "134 loss:  0.019084420055150986 train acc:  0.9994153879818161 micro:  0.5969626168224299\n",
      "135 loss:  0.017059165984392166 train acc:  0.9994153879818161 micro:  0.5922897196261683\n",
      "136 loss:  0.015205372124910355 train acc:  0.9994422714862986 micro:  0.5957943925233645\n",
      "137 loss:  0.01351438369601965 train acc:  0.9994422714862986 micro:  0.5969626168224299\n",
      "138 loss:  0.01198546402156353 train acc:  0.9994422714862986 micro:  0.6004672897196262\n",
      "139 loss:  0.010599225759506226 train acc:  0.9997211430060354 micro:  0.6004672897196262\n",
      "140 loss:  0.00936567597091198 train acc:  0.9997211430060354 micro:  0.6004672897196262\n",
      "141 loss:  0.008275722153484821 train acc:  0.99969436899659 micro:  0.602803738317757\n",
      "142 loss:  0.0073361643590033054 train acc:  0.99969436899659 micro:  0.5992990654205608\n",
      "143 loss:  0.00654265284538269 train acc:  0.99969436899659 micro:  0.5992990654205608\n",
      "144 loss:  0.005867579486221075 train acc:  0.99969436899659 micro:  0.6004672897196262\n",
      "145 loss:  0.0052586449310183525 train acc:  0.99969436899659 micro:  0.6004672897196262\n",
      "146 loss:  0.004711692221462727 train acc:  1.0 micro:  0.5969626168224299\n",
      "147 loss:  0.004227234981954098 train acc:  1.0 micro:  0.5981308411214953\n",
      "148 loss:  0.003806948894634843 train acc:  1.0 micro:  0.5957943925233645\n",
      "149 loss:  0.0034924913197755814 train acc:  1.0 micro:  0.5969626168224299\n",
      "150 loss:  0.0032551749609410763 train acc:  1.0 micro:  0.5969626168224299\n",
      "151 loss:  0.0030623204074800014 train acc:  1.0 micro:  0.5981308411214953\n",
      "152 loss:  0.002911760238930583 train acc:  1.0 micro:  0.5969626168224299\n",
      "153 loss:  0.0028014357667416334 train acc:  1.0 micro:  0.5969626168224299\n",
      "154 loss:  0.0027235280722379684 train acc:  1.0 micro:  0.5969626168224299\n",
      "155 loss:  0.0026773735880851746 train acc:  1.0 micro:  0.5969626168224299\n",
      "156 loss:  0.002657112665474415 train acc:  1.0 micro:  0.5969626168224299\n",
      "157 loss:  0.002657078206539154 train acc:  1.0 micro:  0.5969626168224299\n",
      "158 loss:  0.002672012662515044 train acc:  1.0 micro:  0.5981308411214953\n",
      "159 loss:  0.0027002296410501003 train acc:  1.0 micro:  0.5969626168224299\n",
      "160 loss:  0.0027375174686312675 train acc:  1.0 micro:  0.594626168224299\n",
      "161 loss:  0.0027807888109236956 train acc:  1.0 micro:  0.5922897196261683\n",
      "162 loss:  0.002827136078849435 train acc:  1.0 micro:  0.5934579439252337\n",
      "163 loss:  0.002872050739824772 train acc:  1.0 micro:  0.5911214953271028\n",
      "164 loss:  0.0029159579426050186 train acc:  1.0 micro:  0.5899532710280374\n",
      "165 loss:  0.002960317302495241 train acc:  1.0 micro:  0.5899532710280374\n",
      "166 loss:  0.0030048745684325695 train acc:  1.0 micro:  0.5899532710280374\n",
      "167 loss:  0.003049729857593775 train acc:  1.0 micro:  0.5911214953271028\n",
      "168 loss:  0.003092558356001973 train acc:  1.0 micro:  0.5934579439252337\n",
      "169 loss:  0.0031336138490587473 train acc:  1.0 micro:  0.5969626168224299\n",
      "170 loss:  0.0031746786553412676 train acc:  1.0 micro:  0.5969626168224299\n",
      "171 loss:  0.0032142375130206347 train acc:  1.0 micro:  0.5957943925233645\n",
      "172 loss:  0.003251388669013977 train acc:  1.0 micro:  0.5969626168224299\n",
      "173 loss:  0.0032868366688489914 train acc:  1.0 micro:  0.5969626168224299\n",
      "174 loss:  0.0033204560168087482 train acc:  1.0 micro:  0.5969626168224299\n",
      "175 loss:  0.0033513789530843496 train acc:  1.0 micro:  0.5957943925233645\n",
      "176 loss:  0.003380537498742342 train acc:  1.0 micro:  0.5969626168224299\n",
      "177 loss:  0.0034061428159475327 train acc:  1.0 micro:  0.5981308411214953\n",
      "178 loss:  0.003426393261179328 train acc:  1.0 micro:  0.5981308411214953\n",
      "179 loss:  0.0034422376193106174 train acc:  1.0 micro:  0.5981308411214953\n",
      "180 loss:  0.003454065416008234 train acc:  1.0 micro:  0.5981308411214953\n",
      "181 loss:  0.0034618801437318325 train acc:  1.0 micro:  0.5992990654205608\n",
      "182 loss:  0.0034658650401979685 train acc:  1.0 micro:  0.5981308411214953\n",
      "183 loss:  0.0034659849479794502 train acc:  1.0 micro:  0.5992990654205608\n",
      "184 loss:  0.0034632678143680096 train acc:  1.0 micro:  0.6016355140186916\n",
      "185 loss:  0.0034587387926876545 train acc:  1.0 micro:  0.6004672897196262\n",
      "186 loss:  0.0034539520274847746 train acc:  1.0 micro:  0.6016355140186916\n",
      "187 loss:  0.003449039999395609 train acc:  1.0 micro:  0.602803738317757\n",
      "188 loss:  0.0034425947815179825 train acc:  1.0 micro:  0.6051401869158879\n",
      "189 loss:  0.003435015445575118 train acc:  1.0 micro:  0.6016355140186916\n",
      "190 loss:  0.00342713319696486 train acc:  1.0 micro:  0.6016355140186916\n",
      "191 loss:  0.003418322652578354 train acc:  1.0 micro:  0.6004672897196262\n",
      "192 loss:  0.0034092608839273453 train acc:  1.0 micro:  0.6004672897196262\n",
      "193 loss:  0.0033996966667473316 train acc:  1.0 micro:  0.6016355140186916\n",
      "194 loss:  0.0033897324465215206 train acc:  1.0 micro:  0.6016355140186916\n",
      "195 loss:  0.0033795482013374567 train acc:  1.0 micro:  0.6016355140186916\n",
      "196 loss:  0.0033687897957861423 train acc:  1.0 micro:  0.6016355140186916\n",
      "197 loss:  0.003351927734911442 train acc:  1.0 micro:  0.6039719626168224\n",
      "198 loss:  0.0033304712269455194 train acc:  1.0 micro:  0.6051401869158879\n",
      "199 loss:  0.0033051318023353815 train acc:  1.0 micro:  0.6039719626168224\n",
      "200 loss:  0.003272762056440115 train acc:  1.0 micro:  0.602803738317757\n",
      "201 loss:  0.0032373962458223104 train acc:  1.0 micro:  0.6004672897196262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 loss:  0.0031989291310310364 train acc:  1.0 micro:  0.5992990654205608\n",
      "203 loss:  0.003158093662932515 train acc:  1.0 micro:  0.5992990654205608\n",
      "204 loss:  0.0031182279344648123 train acc:  1.0 micro:  0.5981308411214953\n",
      "205 loss:  0.003085486823692918 train acc:  1.0 micro:  0.5969626168224299\n",
      "206 loss:  0.0030639031901955605 train acc:  1.0 micro:  0.5957943925233645\n",
      "207 loss:  0.0030541103333234787 train acc:  1.0 micro:  0.5957943925233645\n",
      "208 loss:  0.0030547003261744976 train acc:  1.0 micro:  0.5957943925233645\n",
      "209 loss:  0.003056885441765189 train acc:  1.0 micro:  0.5969626168224299\n",
      "210 loss:  0.003064032644033432 train acc:  1.0 micro:  0.5957943925233645\n",
      "211 loss:  0.0030712569132447243 train acc:  1.0 micro:  0.594626168224299\n",
      "212 loss:  0.003076865104958415 train acc:  1.0 micro:  0.594626168224299\n",
      "213 loss:  0.0030839608516544104 train acc:  1.0 micro:  0.5934579439252337\n",
      "214 loss:  0.003085966920480132 train acc:  1.0 micro:  0.594626168224299\n",
      "215 loss:  0.0030923294834792614 train acc:  1.0 micro:  0.5934579439252337\n",
      "216 loss:  0.0030895934905856848 train acc:  1.0 micro:  0.5957943925233645\n",
      "217 loss:  0.003088998841121793 train acc:  1.0 micro:  0.5911214953271028\n",
      "218 loss:  0.003091770224273205 train acc:  1.0 micro:  0.5922897196261683\n",
      "219 loss:  0.0030911765061318874 train acc:  1.0 micro:  0.5899532710280374\n",
      "220 loss:  0.0030935255344957113 train acc:  1.0 micro:  0.5887850467289719\n",
      "221 loss:  0.003092354629188776 train acc:  1.0 micro:  0.5887850467289719\n",
      "222 loss:  0.0030931190121918917 train acc:  1.0 micro:  0.5876168224299065\n",
      "223 loss:  0.003096249420195818 train acc:  1.0 micro:  0.5864485981308412\n",
      "224 loss:  0.003095254534855485 train acc:  1.0 micro:  0.5841121495327103\n",
      "225 loss:  0.0030980673618614674 train acc:  1.0 micro:  0.5899532710280374\n",
      "226 loss:  0.0030978641007095575 train acc:  1.0 micro:  0.5852803738317757\n",
      "227 loss:  0.0031013591215014458 train acc:  1.0 micro:  0.5887850467289719\n",
      "228 loss:  0.0030989903025329113 train acc:  1.0 micro:  0.5864485981308412\n",
      "229 loss:  0.0030971423257142305 train acc:  1.0 micro:  0.5864485981308412\n",
      "230 loss:  0.0030963923782110214 train acc:  1.0 micro:  0.5876168224299065\n",
      "231 loss:  0.0030972189269959927 train acc:  1.0 micro:  0.5876168224299065\n",
      "232 loss:  0.0030967171769589186 train acc:  1.0 micro:  0.5887850467289719\n",
      "233 loss:  0.0030944065656512976 train acc:  1.0 micro:  0.5876168224299065\n",
      "234 loss:  0.0030995693523436785 train acc:  1.0 micro:  0.5864485981308412\n",
      "235 loss:  0.0030987709760665894 train acc:  1.0 micro:  0.5887850467289719\n",
      "236 loss:  0.0030968328937888145 train acc:  1.0 micro:  0.5864485981308412\n",
      "237 loss:  0.0030875690281391144 train acc:  1.0 micro:  0.5852803738317757\n",
      "238 loss:  0.0030860784463584423 train acc:  1.0 micro:  0.5899532710280374\n",
      "239 loss:  0.0030879823025316 train acc:  1.0 micro:  0.5876168224299065\n",
      "240 loss:  0.0030898083932697773 train acc:  1.0 micro:  0.5876168224299065\n",
      "241 loss:  0.003095687134191394 train acc:  1.0 micro:  0.5876168224299065\n",
      "242 loss:  0.003095455700531602 train acc:  1.0 micro:  0.5852803738317757\n",
      "243 loss:  0.0030909287743270397 train acc:  1.0 micro:  0.5841121495327103\n",
      "244 loss:  0.00308959255926311 train acc:  1.0 micro:  0.5887850467289719\n",
      "245 loss:  0.0030903425067663193 train acc:  1.0 micro:  0.5864485981308412\n",
      "246 loss:  0.0030898472759872675 train acc:  1.0 micro:  0.5841121495327103\n",
      "247 loss:  0.0030916135292500257 train acc:  1.0 micro:  0.5864485981308412\n",
      "248 loss:  0.003092608880251646 train acc:  1.0 micro:  0.5829439252336449\n",
      "249 loss:  0.0030919420532882214 train acc:  1.0 micro:  0.5852803738317757\n",
      "250 loss:  0.003090194659307599 train acc:  1.0 micro:  0.5817757009345794\n",
      "251 loss:  0.003083048854023218 train acc:  1.0 micro:  0.5841121495327103\n",
      "252 loss:  0.0030751307494938374 train acc:  1.0 micro:  0.5852803738317757\n",
      "253 loss:  0.0030722138471901417 train acc:  1.0 micro:  0.5829439252336449\n",
      "254 loss:  0.0030704254750162363 train acc:  1.0 micro:  0.5852803738317757\n",
      "255 loss:  0.003070131177082658 train acc:  1.0 micro:  0.5817757009345794\n",
      "256 loss:  0.003064145101234317 train acc:  1.0 micro:  0.5852803738317757\n",
      "257 loss:  0.003052070736885071 train acc:  1.0 micro:  0.580607476635514\n",
      "258 loss:  0.00304140942171216 train acc:  1.0 micro:  0.5829439252336449\n",
      "259 loss:  0.0030317436903715134 train acc:  1.0 micro:  0.5841121495327103\n",
      "260 loss:  0.0030260118655860424 train acc:  1.0 micro:  0.5794392523364486\n",
      "261 loss:  0.0030221459455788136 train acc:  1.0 micro:  0.5841121495327103\n",
      "262 loss:  0.0030108406208455563 train acc:  1.0 micro:  0.580607476635514\n",
      "263 loss:  0.0029968470335006714 train acc:  1.0 micro:  0.5829439252336449\n",
      "264 loss:  0.0029825663659721613 train acc:  1.0 micro:  0.580607476635514\n",
      "265 loss:  0.0029673727694898844 train acc:  1.0 micro:  0.5794392523364486\n",
      "266 loss:  0.002955540083348751 train acc:  1.0 micro:  0.5817757009345794\n",
      "267 loss:  0.0029446128755807877 train acc:  1.0 micro:  0.5771028037383178\n",
      "268 loss:  0.002932871226221323 train acc:  1.0 micro:  0.580607476635514\n",
      "269 loss:  0.0029193637892603874 train acc:  1.0 micro:  0.5794392523364486\n",
      "270 loss:  0.002906364854425192 train acc:  1.0 micro:  0.5771028037383178\n",
      "271 loss:  0.002888774499297142 train acc:  1.0 micro:  0.5782710280373832\n",
      "272 loss:  0.0028749839402735233 train acc:  1.0 micro:  0.5771028037383178\n",
      "273 loss:  0.0028647121507674456 train acc:  1.0 micro:  0.5735981308411215\n",
      "274 loss:  0.0028518186882138252 train acc:  1.0 micro:  0.5759345794392523\n",
      "275 loss:  0.002842996269464493 train acc:  1.0 micro:  0.5747663551401869\n",
      "276 loss:  0.0028306881431490183 train acc:  1.0 micro:  0.5747663551401869\n",
      "277 loss:  0.002819012152031064 train acc:  1.0 micro:  0.5747663551401869\n",
      "278 loss:  0.0028081010095775127 train acc:  1.0 micro:  0.5759345794392523\n",
      "279 loss:  0.0027988667134195566 train acc:  1.0 micro:  0.5735981308411215\n",
      "280 loss:  0.0027893134392797947 train acc:  1.0 micro:  0.5759345794392523\n",
      "281 loss:  0.0027822169940918684 train acc:  1.0 micro:  0.5759345794392523\n",
      "282 loss:  0.0027779529336839914 train acc:  1.0 micro:  0.572429906542056\n",
      "283 loss:  0.0027707398403435946 train acc:  1.0 micro:  0.5747663551401869\n",
      "284 loss:  0.0027680026832967997 train acc:  1.0 micro:  0.5747663551401869\n",
      "285 loss:  0.0027627579402178526 train acc:  1.0 micro:  0.5747663551401869\n",
      "286 loss:  0.0027586331125348806 train acc:  1.0 micro:  0.5747663551401869\n",
      "287 loss:  0.0027554130647331476 train acc:  1.0 micro:  0.5747663551401869\n",
      "288 loss:  0.002753912005573511 train acc:  1.0 micro:  0.5712616822429907\n",
      "289 loss:  0.002751377411186695 train acc:  1.0 micro:  0.5735981308411215\n",
      "290 loss:  0.002753796987235546 train acc:  1.0 micro:  0.5689252336448598\n",
      "291 loss:  0.002748735249042511 train acc:  1.0 micro:  0.5700934579439252\n",
      "292 loss:  0.002748187631368637 train acc:  1.0 micro:  0.5689252336448598\n",
      "293 loss:  0.0027464234735816717 train acc:  1.0 micro:  0.5677570093457944\n",
      "294 loss:  0.002745887031778693 train acc:  1.0 micro:  0.5677570093457944\n",
      "295 loss:  0.0027444851584732533 train acc:  1.0 micro:  0.5677570093457944\n",
      "296 loss:  0.002746445359662175 train acc:  1.0 micro:  0.5689252336448598\n",
      "297 loss:  0.0027403475251048803 train acc:  1.0 micro:  0.5665887850467289\n",
      "298 loss:  0.0027403475251048803 train acc:  1.0 micro:  0.5654205607476636\n",
      "299 loss:  0.002734116045758128 train acc:  1.0 micro:  0.5654205607476636\n",
      "300 loss:  0.002734856680035591 train acc:  1.0 micro:  0.5642523364485982\n",
      "301 loss:  0.002732975874096155 train acc:  1.0 micro:  0.5642523364485982\n",
      "302 loss:  0.002733865985646844 train acc:  1.0 micro:  0.5689252336448598\n",
      "303 loss:  0.002740380819886923 train acc:  1.0 micro:  0.5654205607476636\n",
      "304 loss:  0.002737862290814519 train acc:  1.0 micro:  0.5689252336448598\n",
      "305 loss:  0.0027399729005992413 train acc:  1.0 micro:  0.5630841121495327\n",
      "306 loss:  0.002729602623730898 train acc:  1.0 micro:  0.5642523364485982\n",
      "307 loss:  0.0027265395037829876 train acc:  1.0 micro:  0.5654205607476636\n",
      "308 loss:  0.0027284903917461634 train acc:  1.0 micro:  0.5642523364485982\n",
      "309 loss:  0.0027310308068990707 train acc:  1.0 micro:  0.5654205607476636\n",
      "310 loss:  0.0027389475144445896 train acc:  1.0 micro:  0.5642523364485982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 loss:  0.002735845511779189 train acc:  1.0 micro:  0.5642523364485982\n",
      "312 loss:  0.0027314440812915564 train acc:  1.0 micro:  0.5654205607476636\n",
      "313 loss:  0.0027252330910414457 train acc:  1.0 micro:  0.5654205607476636\n",
      "314 loss:  0.0027227397076785564 train acc:  1.0 micro:  0.5642523364485982\n",
      "315 loss:  0.0027297120541334152 train acc:  1.0 micro:  0.5677570093457944\n",
      "316 loss:  0.0027313397731631994 train acc:  1.0 micro:  0.5630841121495327\n",
      "317 loss:  0.0027313667815178633 train acc:  1.0 micro:  0.5654205607476636\n",
      "318 loss:  0.0027277835179120302 train acc:  1.0 micro:  0.5665887850467289\n",
      "319 loss:  0.0027272335719317198 train acc:  1.0 micro:  0.5665887850467289\n",
      "320 loss:  0.002726905746385455 train acc:  1.0 micro:  0.5654205607476636\n",
      "321 loss:  0.0027295267209410667 train acc:  1.0 micro:  0.5642523364485982\n",
      "322 loss:  0.002729845931753516 train acc:  1.0 micro:  0.5665887850467289\n",
      "323 loss:  0.0027283714152872562 train acc:  1.0 micro:  0.5630841121495327\n",
      "324 loss:  0.002735218033194542 train acc:  1.0 micro:  0.5665887850467289\n",
      "325 loss:  0.0027320587541908026 train acc:  1.0 micro:  0.5654205607476636\n",
      "326 loss:  0.0027369149029254913 train acc:  1.0 micro:  0.5642523364485982\n",
      "327 loss:  0.002724123653024435 train acc:  1.0 micro:  0.5654205607476636\n",
      "328 loss:  0.002722097560763359 train acc:  1.0 micro:  0.5665887850467289\n",
      "329 loss:  0.0027264095842838287 train acc:  1.0 micro:  0.5630841121495327\n",
      "330 loss:  0.0027227820828557014 train acc:  1.0 micro:  0.5654205607476636\n",
      "331 loss:  0.0027289697900414467 train acc:  1.0 micro:  0.5654205607476636\n",
      "332 loss:  0.0027276400942355394 train acc:  1.0 micro:  0.5654205607476636\n",
      "333 loss:  0.002729659667238593 train acc:  1.0 micro:  0.5654205607476636\n",
      "334 loss:  0.0027247711550444365 train acc:  1.0 micro:  0.5677570093457944\n",
      "335 loss:  0.002723133424296975 train acc:  1.0 micro:  0.5677570093457944\n",
      "336 loss:  0.0027242579963058233 train acc:  1.0 micro:  0.5642523364485982\n",
      "337 loss:  0.0027253946755081415 train acc:  1.0 micro:  0.5665887850467289\n",
      "338 loss:  0.002732133725658059 train acc:  1.0 micro:  0.5642523364485982\n",
      "339 loss:  0.0027297749184072018 train acc:  1.0 micro:  0.5677570093457944\n",
      "340 loss:  0.0027348087169229984 train acc:  1.0 micro:  0.5665887850467289\n",
      "341 loss:  0.002721409546211362 train acc:  1.0 micro:  0.5654205607476636\n",
      "342 loss:  0.002719542942941189 train acc:  1.0 micro:  0.5689252336448598\n",
      "343 loss:  0.0027254794258624315 train acc:  1.0 micro:  0.5642523364485982\n",
      "344 loss:  0.0027221704367548227 train acc:  1.0 micro:  0.5665887850467289\n",
      "345 loss:  0.0027245283126831055 train acc:  1.0 micro:  0.5630841121495327\n",
      "346 loss:  0.002719069831073284 train acc:  1.0 micro:  0.5642523364485982\n",
      "347 loss:  0.00271808123216033 train acc:  1.0 micro:  0.5654205607476636\n",
      "348 loss:  0.0027195948641747236 train acc:  1.0 micro:  0.5642523364485982\n",
      "349 loss:  0.002717869821935892 train acc:  1.0 micro:  0.5665887850467289\n",
      "350 loss:  0.0027220454066991806 train acc:  1.0 micro:  0.5642523364485982\n",
      "351 loss:  0.0027206174563616514 train acc:  1.0 micro:  0.5665887850467289\n",
      "352 loss:  0.002721651690080762 train acc:  1.0 micro:  0.5654205607476636\n",
      "353 loss:  0.0027173268608748913 train acc:  1.0 micro:  0.5665887850467289\n",
      "354 loss:  0.0027147107757627964 train acc:  1.0 micro:  0.5665887850467289\n",
      "355 loss:  0.0027151755057275295 train acc:  1.0 micro:  0.5677570093457944\n",
      "356 loss:  0.002714119851589203 train acc:  1.0 micro:  0.5665887850467289\n",
      "357 loss:  0.002724103629589081 train acc:  1.0 micro:  0.5654205607476636\n",
      "358 loss:  0.002719121053814888 train acc:  1.0 micro:  0.5677570093457944\n",
      "359 loss:  0.00272125075571239 train acc:  1.0 micro:  0.5654205607476636\n",
      "360 loss:  0.002712095621973276 train acc:  1.0 micro:  0.5654205607476636\n",
      "361 loss:  0.0027088283095508814 train acc:  1.0 micro:  0.5665887850467289\n",
      "362 loss:  0.002712076297029853 train acc:  1.0 micro:  0.5665887850467289\n",
      "363 loss:  0.002711850916966796 train acc:  1.0 micro:  0.5689252336448598\n",
      "364 loss:  0.002718233037739992 train acc:  1.0 micro:  0.5700934579439252\n",
      "365 loss:  0.0027124916668981314 train acc:  1.0 micro:  0.5689252336448598\n",
      "366 loss:  0.002711792942136526 train acc:  1.0 micro:  0.5654205607476636\n",
      "367 loss:  0.0027051803190261126 train acc:  1.0 micro:  0.5677570093457944\n",
      "368 loss:  0.0027056217659264803 train acc:  1.0 micro:  0.5689252336448598\n",
      "369 loss:  0.002709418535232544 train acc:  1.0 micro:  0.5700934579439252\n",
      "370 loss:  0.0027078832499682903 train acc:  1.0 micro:  0.5689252336448598\n",
      "371 loss:  0.0027134513948112726 train acc:  1.0 micro:  0.5700934579439252\n",
      "372 loss:  0.00270599196664989 train acc:  1.0 micro:  0.5689252336448598\n",
      "373 loss:  0.002706041093915701 train acc:  1.0 micro:  0.5677570093457944\n",
      "374 loss:  0.0027012708596885204 train acc:  1.0 micro:  0.5689252336448598\n",
      "375 loss:  0.0027006755117326975 train acc:  1.0 micro:  0.5677570093457944\n",
      "376 loss:  0.0027064664755016565 train acc:  1.0 micro:  0.5689252336448598\n",
      "377 loss:  0.0027017139364033937 train acc:  1.0 micro:  0.5677570093457944\n",
      "378 loss:  0.002704995684325695 train acc:  1.0 micro:  0.5689252336448598\n",
      "379 loss:  0.0026987912133336067 train acc:  1.0 micro:  0.5677570093457944\n",
      "380 loss:  0.002697784686461091 train acc:  1.0 micro:  0.5700934579439252\n",
      "381 loss:  0.0026976470835506916 train acc:  1.0 micro:  0.5700934579439252\n",
      "382 loss:  0.0026965891011059284 train acc:  1.0 micro:  0.5700934579439252\n",
      "383 loss:  0.002703216392546892 train acc:  1.0 micro:  0.5689252336448598\n",
      "384 loss:  0.0027001153212040663 train acc:  1.0 micro:  0.5677570093457944\n",
      "385 loss:  0.0027038785628974438 train acc:  1.0 micro:  0.5700934579439252\n",
      "386 loss:  0.0026984356809407473 train acc:  1.0 micro:  0.5712616822429907\n",
      "387 loss:  0.0026960682589560747 train acc:  1.0 micro:  0.572429906542056\n",
      "388 loss:  0.0026949318125844 train acc:  1.0 micro:  0.5700934579439252\n",
      "389 loss:  0.0026920244563370943 train acc:  1.0 micro:  0.5700934579439252\n",
      "390 loss:  0.0026968198362737894 train acc:  1.0 micro:  0.5689252336448598\n",
      "391 loss:  0.0026927064172923565 train acc:  1.0 micro:  0.5712616822429907\n",
      "392 loss:  0.0026953364722430706 train acc:  1.0 micro:  0.5700934579439252\n",
      "393 loss:  0.0026918333023786545 train acc:  1.0 micro:  0.5712616822429907\n",
      "394 loss:  0.002690565539523959 train acc:  1.0 micro:  0.5712616822429907\n",
      "395 loss:  0.002690107561647892 train acc:  1.0 micro:  0.5689252336448598\n",
      "396 loss:  0.0026888169813901186 train acc:  1.0 micro:  0.5712616822429907\n",
      "397 loss:  0.0026947578880935907 train acc:  1.0 micro:  0.5677570093457944\n",
      "398 loss:  0.0026912393514066935 train acc:  1.0 micro:  0.5700934579439252\n",
      "399 loss:  0.0026950854808092117 train acc:  1.0 micro:  0.5689252336448598\n",
      "400 loss:  0.002688064007088542 train acc:  1.0 micro:  0.5712616822429907\n",
      "401 loss:  0.0026863152161240578 train acc:  1.0 micro:  0.5700934579439252\n",
      "402 loss:  0.002687063766643405 train acc:  1.0 micro:  0.5689252336448598\n",
      "403 loss:  0.0026854148600250483 train acc:  1.0 micro:  0.5689252336448598\n",
      "404 loss:  0.002691230969503522 train acc:  1.0 micro:  0.5689252336448598\n",
      "405 loss:  0.002688156208023429 train acc:  1.0 micro:  0.5700934579439252\n",
      "406 loss:  0.0026906465645879507 train acc:  1.0 micro:  0.5689252336448598\n",
      "407 loss:  0.0026863275561481714 train acc:  1.0 micro:  0.5677570093457944\n",
      "408 loss:  0.0026836618781089783 train acc:  1.0 micro:  0.5689252336448598\n",
      "409 loss:  0.0026857610791921616 train acc:  1.0 micro:  0.5677570093457944\n",
      "410 loss:  0.002684104023501277 train acc:  1.0 micro:  0.5700934579439252\n",
      "411 loss:  0.0026905385311692953 train acc:  1.0 micro:  0.5700934579439252\n",
      "412 loss:  0.0026849713176488876 train acc:  1.0 micro:  0.5677570093457944\n",
      "413 loss:  0.0026859052013605833 train acc:  1.0 micro:  0.5700934579439252\n",
      "414 loss:  0.002681694459170103 train acc:  1.0 micro:  0.5654205607476636\n",
      "415 loss:  0.002679195487871766 train acc:  1.0 micro:  0.572429906542056\n",
      "416 loss:  0.0026836651377379894 train acc:  1.0 micro:  0.5677570093457944\n",
      "417 loss:  0.0026793903671205044 train acc:  1.0 micro:  0.5654205607476636\n",
      "418 loss:  0.002681355457752943 train acc:  1.0 micro:  0.5689252336448598\n",
      "419 loss:  0.002678545657545328 train acc:  1.0 micro:  0.5642523364485982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 loss:  0.0026769577525556087 train acc:  1.0 micro:  0.5677570093457944\n",
      "421 loss:  0.0026777025777846575 train acc:  1.0 micro:  0.5654205607476636\n",
      "422 loss:  0.002675762167200446 train acc:  1.0 micro:  0.5689252336448598\n",
      "423 loss:  0.0026824523229151964 train acc:  1.0 micro:  0.5665887850467289\n",
      "424 loss:  0.0026775638107210398 train acc:  1.0 micro:  0.5642523364485982\n",
      "425 loss:  0.002679102122783661 train acc:  1.0 micro:  0.5665887850467289\n",
      "426 loss:  0.002674645744264126 train acc:  1.0 micro:  0.5665887850467289\n",
      "427 loss:  0.002671360271051526 train acc:  1.0 micro:  0.5689252336448598\n",
      "428 loss:  0.002671268070116639 train acc:  1.0 micro:  0.5665887850467289\n",
      "429 loss:  0.0026688070502132177 train acc:  1.0 micro:  0.5677570093457944\n",
      "430 loss:  0.002674225950613618 train acc:  1.0 micro:  0.5677570093457944\n",
      "431 loss:  0.0026703188195824623 train acc:  1.0 micro:  0.5654205607476636\n",
      "432 loss:  0.0026744252536445856 train acc:  1.0 micro:  0.5677570093457944\n",
      "433 loss:  0.0026663814205676317 train acc:  1.0 micro:  0.5677570093457944\n",
      "434 loss:  0.0026657232083380222 train acc:  1.0 micro:  0.5665887850467289\n",
      "435 loss:  0.0026666815392673016 train acc:  1.0 micro:  0.5677570093457944\n",
      "436 loss:  0.0026645574253052473 train acc:  1.0 micro:  0.5665887850467289\n",
      "437 loss:  0.0026685630436986685 train acc:  1.0 micro:  0.5677570093457944\n",
      "438 loss:  0.0026622870936989784 train acc:  1.0 micro:  0.5654205607476636\n",
      "439 loss:  0.002662316197529435 train acc:  1.0 micro:  0.5677570093457944\n",
      "440 loss:  0.002659787191078067 train acc:  1.0 micro:  0.5665887850467289\n",
      "441 loss:  0.002658709418028593 train acc:  1.0 micro:  0.5665887850467289\n",
      "442 loss:  0.0026618638075888157 train acc:  1.0 micro:  0.5677570093457944\n",
      "443 loss:  0.0026592945214360952 train acc:  1.0 micro:  0.5654205607476636\n",
      "444 loss:  0.002664253581315279 train acc:  1.0 micro:  0.5677570093457944\n",
      "445 loss:  0.0026576181408017874 train acc:  1.0 micro:  0.5654205607476636\n",
      "446 loss:  0.002657045144587755 train acc:  1.0 micro:  0.5677570093457944\n",
      "447 loss:  0.002654343144968152 train acc:  1.0 micro:  0.5665887850467289\n",
      "448 loss:  0.0026523759588599205 train acc:  1.0 micro:  0.5665887850467289\n",
      "449 loss:  0.0026567073073238134 train acc:  1.0 micro:  0.5677570093457944\n",
      "450 loss:  0.0026521049439907074 train acc:  1.0 micro:  0.5665887850467289\n",
      "451 loss:  0.0026541058905422688 train acc:  1.0 micro:  0.5665887850467289\n",
      "452 loss:  0.002650662325322628 train acc:  1.0 micro:  0.5642523364485982\n",
      "453 loss:  0.0026485684793442488 train acc:  1.0 micro:  0.5665887850467289\n",
      "454 loss:  0.0026471801102161407 train acc:  1.0 micro:  0.5630841121495327\n",
      "455 loss:  0.002644519554451108 train acc:  1.0 micro:  0.5654205607476636\n",
      "456 loss:  0.002648772671818733 train acc:  1.0 micro:  0.5665887850467289\n",
      "457 loss:  0.002646173583343625 train acc:  1.0 micro:  0.5677570093457944\n",
      "458 loss:  0.0026473109610378742 train acc:  1.0 micro:  0.5665887850467289\n",
      "459 loss:  0.00264176563359797 train acc:  1.0 micro:  0.5665887850467289\n",
      "460 loss:  0.0026430278085172176 train acc:  1.0 micro:  0.5642523364485982\n",
      "461 loss:  0.0026406694669276476 train acc:  1.0 micro:  0.5654205607476636\n",
      "462 loss:  0.002642426872625947 train acc:  1.0 micro:  0.5654205607476636\n",
      "463 loss:  0.002640169346705079 train acc:  1.0 micro:  0.5654205607476636\n",
      "464 loss:  0.002638180274516344 train acc:  1.0 micro:  0.5654205607476636\n",
      "465 loss:  0.0026418217457830906 train acc:  1.0 micro:  0.5665887850467289\n",
      "466 loss:  0.002637162571772933 train acc:  1.0 micro:  0.5654205607476636\n",
      "467 loss:  0.002638895995914936 train acc:  1.0 micro:  0.5654205607476636\n",
      "468 loss:  0.0026339280884712934 train acc:  1.0 micro:  0.5654205607476636\n",
      "469 loss:  0.0026326244696974754 train acc:  1.0 micro:  0.5642523364485982\n",
      "470 loss:  0.002638050355017185 train acc:  1.0 micro:  0.5654205607476636\n",
      "471 loss:  0.0026341008488088846 train acc:  1.0 micro:  0.5654205607476636\n",
      "472 loss:  0.002636859891936183 train acc:  1.0 micro:  0.5642523364485982\n",
      "473 loss:  0.0026316773146390915 train acc:  1.0 micro:  0.5642523364485982\n",
      "474 loss:  0.002629227703437209 train acc:  1.0 micro:  0.5642523364485982\n",
      "475 loss:  0.002629287075251341 train acc:  1.0 micro:  0.5642523364485982\n",
      "476 loss:  0.0026262449100613594 train acc:  1.0 micro:  0.5654205607476636\n",
      "477 loss:  0.002628531539812684 train acc:  1.0 micro:  0.5630841121495327\n",
      "478 loss:  0.0026259394362568855 train acc:  1.0 micro:  0.5654205607476636\n",
      "479 loss:  0.002624177373945713 train acc:  1.0 micro:  0.5654205607476636\n",
      "480 loss:  0.002623456297442317 train acc:  1.0 micro:  0.5665887850467289\n",
      "481 loss:  0.0026219298597425222 train acc:  1.0 micro:  0.5665887850467289\n",
      "482 loss:  0.0026293848641216755 train acc:  1.0 micro:  0.5642523364485982\n",
      "483 loss:  0.002623972948640585 train acc:  1.0 micro:  0.5677570093457944\n",
      "484 loss:  0.002622961765155196 train acc:  1.0 micro:  0.5665887850467289\n",
      "485 loss:  0.0026176634710282087 train acc:  1.0 micro:  0.5677570093457944\n",
      "486 loss:  0.0026159423869103193 train acc:  1.0 micro:  0.5677570093457944\n",
      "487 loss:  0.002621868858113885 train acc:  1.0 micro:  0.5665887850467289\n",
      "488 loss:  0.00261706649325788 train acc:  1.0 micro:  0.5677570093457944\n",
      "489 loss:  0.002615799894556403 train acc:  1.0 micro:  0.5677570093457944\n",
      "490 loss:  0.0026123779825866222 train acc:  1.0 micro:  0.5677570093457944\n",
      "491 loss:  0.002611620118841529 train acc:  1.0 micro:  0.5677570093457944\n",
      "492 loss:  0.0026164413429796696 train acc:  1.0 micro:  0.5689252336448598\n",
      "493 loss:  0.0026123186107724905 train acc:  1.0 micro:  0.5677570093457944\n",
      "494 loss:  0.0026129563339054585 train acc:  1.0 micro:  0.5677570093457944\n",
      "495 loss:  0.002606003312394023 train acc:  1.0 micro:  0.5677570093457944\n",
      "496 loss:  0.0026058151852339506 train acc:  1.0 micro:  0.5677570093457944\n",
      "497 loss:  0.002608139533549547 train acc:  1.0 micro:  0.5689252336448598\n",
      "498 loss:  0.0026047169230878353 train acc:  1.0 micro:  0.5677570093457944\n",
      "499 loss:  0.0026076664216816425 train acc:  1.0 micro:  0.5700934579439252\n",
      "500 loss:  0.0026007862761616707 train acc:  1.0 micro:  0.5689252336448598\n",
      "501 loss:  0.0026001057121902704 train acc:  1.0 micro:  0.5677570093457944\n",
      "502 loss:  0.002603658474981785 train acc:  1.0 micro:  0.5700934579439252\n",
      "503 loss:  0.0026004347018897533 train acc:  1.0 micro:  0.5677570093457944\n",
      "504 loss:  0.0026029227301478386 train acc:  1.0 micro:  0.5689252336448598\n",
      "505 loss:  0.002596314763650298 train acc:  1.0 micro:  0.5689252336448598\n",
      "506 loss:  0.0025943631771951914 train acc:  1.0 micro:  0.5677570093457944\n",
      "507 loss:  0.002596440492197871 train acc:  1.0 micro:  0.5689252336448598\n",
      "508 loss:  0.0025936728343367577 train acc:  1.0 micro:  0.5677570093457944\n",
      "509 loss:  0.0025971978902816772 train acc:  1.0 micro:  0.5689252336448598\n",
      "510 loss:  0.0025925645604729652 train acc:  1.0 micro:  0.5689252336448598\n",
      "511 loss:  0.002592404605820775 train acc:  1.0 micro:  0.5689252336448598\n",
      "512 loss:  0.0025891100522130728 train acc:  1.0 micro:  0.5677570093457944\n",
      "513 loss:  0.0025874944403767586 train acc:  1.0 micro:  0.5689252336448598\n",
      "514 loss:  0.0025898132007569075 train acc:  1.0 micro:  0.5677570093457944\n",
      "515 loss:  0.00258762133307755 train acc:  1.0 micro:  0.5677570093457944\n",
      "516 loss:  0.0025917328894138336 train acc:  1.0 micro:  0.5677570093457944\n",
      "517 loss:  0.0025850117672234774 train acc:  1.0 micro:  0.5677570093457944\n",
      "518 loss:  0.0025843833573162556 train acc:  1.0 micro:  0.5689252336448598\n",
      "519 loss:  0.002583014313131571 train acc:  1.0 micro:  0.5677570093457944\n",
      "520 loss:  0.0025805365294218063 train acc:  1.0 micro:  0.5677570093457944\n",
      "521 loss:  0.0025830089580267668 train acc:  1.0 micro:  0.5677570093457944\n",
      "522 loss:  0.0025798166170716286 train acc:  1.0 micro:  0.5677570093457944\n",
      "523 loss:  0.0025807435158640146 train acc:  1.0 micro:  0.5677570093457944\n",
      "524 loss:  0.002576730214059353 train acc:  1.0 micro:  0.5677570093457944\n",
      "525 loss:  0.0025740591809153557 train acc:  1.0 micro:  0.5677570093457944\n",
      "526 loss:  0.0025772119406610727 train acc:  1.0 micro:  0.5677570093457944\n",
      "527 loss:  0.0025750913191586733 train acc:  1.0 micro:  0.5677570093457944\n",
      "528 loss:  0.0025790142826735973 train acc:  1.0 micro:  0.5677570093457944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529 loss:  0.002572221914306283 train acc:  1.0 micro:  0.5677570093457944\n",
      "530 loss:  0.002570608165115118 train acc:  1.0 micro:  0.5677570093457944\n",
      "531 loss:  0.002571535762399435 train acc:  1.0 micro:  0.5665887850467289\n",
      "532 loss:  0.002569034928455949 train acc:  1.0 micro:  0.5677570093457944\n",
      "533 loss:  0.0025711683556437492 train acc:  1.0 micro:  0.5665887850467289\n",
      "534 loss:  0.0025660349056124687 train acc:  1.0 micro:  0.5665887850467289\n",
      "535 loss:  0.0025649964809417725 train acc:  1.0 micro:  0.5677570093457944\n",
      "536 loss:  0.002563971560448408 train acc:  1.0 micro:  0.5665887850467289\n",
      "537 loss:  0.0025636604987084866 train acc:  1.0 micro:  0.5677570093457944\n",
      "538 loss:  0.0025682009290903807 train acc:  1.0 micro:  0.5665887850467289\n",
      "539 loss:  0.002562473528087139 train acc:  1.0 micro:  0.5677570093457944\n",
      "540 loss:  0.0025629715528339148 train acc:  1.0 micro:  0.5677570093457944\n",
      "541 loss:  0.0025572164449840784 train acc:  1.0 micro:  0.5677570093457944\n",
      "542 loss:  0.0025559552013874054 train acc:  1.0 micro:  0.5677570093457944\n",
      "543 loss:  0.002558480715379119 train acc:  1.0 micro:  0.5677570093457944\n",
      "544 loss:  0.002554799197241664 train acc:  1.0 micro:  0.5677570093457944\n",
      "545 loss:  0.002556166611611843 train acc:  1.0 micro:  0.5689252336448598\n",
      "546 loss:  0.002552226884290576 train acc:  1.0 micro:  0.5689252336448598\n",
      "547 loss:  0.0025510364212095737 train acc:  1.0 micro:  0.5677570093457944\n",
      "548 loss:  0.002551835495978594 train acc:  1.0 micro:  0.5677570093457944\n",
      "549 loss:  0.00255079404450953 train acc:  1.0 micro:  0.5677570093457944\n",
      "550 loss:  0.0025558725465089083 train acc:  1.0 micro:  0.5677570093457944\n",
      "551 loss:  0.002550474600866437 train acc:  1.0 micro:  0.5677570093457944\n",
      "552 loss:  0.002550902310758829 train acc:  1.0 micro:  0.5689252336448598\n",
      "553 loss:  0.0025472999550402164 train acc:  1.0 micro:  0.5689252336448598\n",
      "554 loss:  0.0025441322941333055 train acc:  1.0 micro:  0.5689252336448598\n",
      "555 loss:  0.0025455516297370195 train acc:  1.0 micro:  0.5689252336448598\n",
      "556 loss:  0.0025424649938941 train acc:  1.0 micro:  0.5689252336448598\n",
      "557 loss:  0.0025436696596443653 train acc:  1.0 micro:  0.5677570093457944\n",
      "558 loss:  0.002540391404181719 train acc:  1.0 micro:  0.5689252336448598\n",
      "559 loss:  0.002539352048188448 train acc:  1.0 micro:  0.5677570093457944\n",
      "560 loss:  0.002538587199524045 train acc:  1.0 micro:  0.5689252336448598\n",
      "561 loss:  0.0025398090947419405 train acc:  1.0 micro:  0.5654205607476636\n",
      "562 loss:  0.0025380877777934074 train acc:  1.0 micro:  0.5689252336448598\n",
      "563 loss:  0.002537369029596448 train acc:  1.0 micro:  0.5677570093457944\n",
      "564 loss:  0.0025331086944788694 train acc:  1.0 micro:  0.5677570093457944\n",
      "565 loss:  0.0025333543308079243 train acc:  1.0 micro:  0.5677570093457944\n",
      "566 loss:  0.002532794838771224 train acc:  1.0 micro:  0.5677570093457944\n",
      "567 loss:  0.0025319799315184355 train acc:  1.0 micro:  0.5677570093457944\n",
      "568 loss:  0.002529399236664176 train acc:  1.0 micro:  0.5677570093457944\n",
      "569 loss:  0.002528409007936716 train acc:  1.0 micro:  0.5677570093457944\n",
      "570 loss:  0.0025277617387473583 train acc:  1.0 micro:  0.5689252336448598\n",
      "571 loss:  0.002528961980715394 train acc:  1.0 micro:  0.5677570093457944\n",
      "572 loss:  0.0025263233110308647 train acc:  1.0 micro:  0.5689252336448598\n",
      "573 loss:  0.0025258315727114677 train acc:  1.0 micro:  0.5677570093457944\n",
      "574 loss:  0.0025226909201592207 train acc:  1.0 micro:  0.5677570093457944\n",
      "575 loss:  0.002522742608562112 train acc:  1.0 micro:  0.5700934579439252\n",
      "576 loss:  0.0025225169956684113 train acc:  1.0 micro:  0.5665887850467289\n",
      "577 loss:  0.0025227072183042765 train acc:  1.0 micro:  0.5700934579439252\n",
      "578 loss:  0.002518757712095976 train acc:  1.0 micro:  0.5665887850467289\n",
      "579 loss:  0.002520369365811348 train acc:  1.0 micro:  0.5665887850467289\n",
      "580 loss:  0.0025167278945446014 train acc:  1.0 micro:  0.5689252336448598\n",
      "581 loss:  0.0025156920310109854 train acc:  1.0 micro:  0.5665887850467289\n",
      "582 loss:  0.002513687126338482 train acc:  1.0 micro:  0.5677570093457944\n",
      "583 loss:  0.0025113853625953197 train acc:  1.0 micro:  0.5665887850467289\n",
      "584 loss:  0.0025127241387963295 train acc:  1.0 micro:  0.5700934579439252\n",
      "585 loss:  0.002509340876713395 train acc:  1.0 micro:  0.5689252336448598\n",
      "586 loss:  0.0025087466929107904 train acc:  1.0 micro:  0.5665887850467289\n",
      "587 loss:  0.0025094053708016872 train acc:  1.0 micro:  0.5665887850467289\n",
      "588 loss:  0.0025085911620408297 train acc:  1.0 micro:  0.5677570093457944\n",
      "589 loss:  0.0025073387660086155 train acc:  1.0 micro:  0.5665887850467289\n",
      "590 loss:  0.0025072800926864147 train acc:  1.0 micro:  0.5665887850467289\n",
      "591 loss:  0.002504932228475809 train acc:  1.0 micro:  0.5665887850467289\n",
      "592 loss:  0.0025082866195589304 train acc:  1.0 micro:  0.5654205607476636\n",
      "593 loss:  0.0025013580452650785 train acc:  1.0 micro:  0.5654205607476636\n",
      "594 loss:  0.002499919617548585 train acc:  1.0 micro:  0.5665887850467289\n",
      "595 loss:  0.0025005529168993235 train acc:  1.0 micro:  0.5654205607476636\n",
      "596 loss:  0.002497642068192363 train acc:  1.0 micro:  0.5654205607476636\n",
      "597 loss:  0.0024972050450742245 train acc:  1.0 micro:  0.5654205607476636\n",
      "598 loss:  0.002496246248483658 train acc:  1.0 micro:  0.5654205607476636\n",
      "599 loss:  0.002495922613888979 train acc:  1.0 micro:  0.5654205607476636\n",
      "600 loss:  0.0024964564945548773 train acc:  1.0 micro:  0.5654205607476636\n",
      "601 loss:  0.0024946497287601233 train acc:  1.0 micro:  0.5654205607476636\n",
      "602 loss:  0.0024923416785895824 train acc:  1.0 micro:  0.5642523364485982\n",
      "603 loss:  0.002495134947821498 train acc:  1.0 micro:  0.5654205607476636\n",
      "604 loss:  0.0024893544614315033 train acc:  1.0 micro:  0.5654205607476636\n",
      "605 loss:  0.0024885768070816994 train acc:  1.0 micro:  0.5654205607476636\n",
      "606 loss:  0.0024879821576178074 train acc:  1.0 micro:  0.5642523364485982\n",
      "607 loss:  0.002486349083483219 train acc:  1.0 micro:  0.5654205607476636\n",
      "608 loss:  0.0024855639785528183 train acc:  1.0 micro:  0.5654205607476636\n",
      "609 loss:  0.002483563730493188 train acc:  1.0 micro:  0.5642523364485982\n",
      "610 loss:  0.0024829020258039236 train acc:  1.0 micro:  0.5654205607476636\n",
      "611 loss:  0.0024826815351843834 train acc:  1.0 micro:  0.5642523364485982\n",
      "612 loss:  0.002480959752574563 train acc:  1.0 micro:  0.5642523364485982\n",
      "613 loss:  0.002479671500623226 train acc:  1.0 micro:  0.5654205607476636\n",
      "614 loss:  0.0024779511149972677 train acc:  1.0 micro:  0.5654205607476636\n",
      "615 loss:  0.0024784491397440434 train acc:  1.0 micro:  0.5642523364485982\n",
      "616 loss:  0.002477793488651514 train acc:  1.0 micro:  0.5665887850467289\n",
      "617 loss:  0.0024846941232681274 train acc:  1.0 micro:  0.5642523364485982\n",
      "618 loss:  0.002475920133292675 train acc:  1.0 micro:  0.5654205607476636\n",
      "619 loss:  0.002473934320732951 train acc:  1.0 micro:  0.5654205607476636\n",
      "620 loss:  0.0024731308221817017 train acc:  1.0 micro:  0.5630841121495327\n",
      "621 loss:  0.0024716267362236977 train acc:  1.0 micro:  0.5642523364485982\n",
      "622 loss:  0.0024726935662329197 train acc:  1.0 micro:  0.5654205607476636\n",
      "623 loss:  0.002469522412866354 train acc:  1.0 micro:  0.5642523364485982\n",
      "624 loss:  0.002467457903549075 train acc:  1.0 micro:  0.5654205607476636\n",
      "625 loss:  0.0024687128607183695 train acc:  1.0 micro:  0.5642523364485982\n",
      "626 loss:  0.0024655102752149105 train acc:  1.0 micro:  0.5654205607476636\n",
      "627 loss:  0.002466654172167182 train acc:  1.0 micro:  0.5642523364485982\n",
      "628 loss:  0.0024623717181384563 train acc:  1.0 micro:  0.5642523364485982\n",
      "629 loss:  0.002461952855810523 train acc:  1.0 micro:  0.5642523364485982\n",
      "630 loss:  0.002463456941768527 train acc:  1.0 micro:  0.5642523364485982\n",
      "631 loss:  0.0024619880132377148 train acc:  1.0 micro:  0.5654205607476636\n",
      "632 loss:  0.002463940531015396 train acc:  1.0 micro:  0.5630841121495327\n",
      "633 loss:  0.0024584834463894367 train acc:  1.0 micro:  0.5642523364485982\n",
      "634 loss:  0.002457206603139639 train acc:  1.0 micro:  0.5654205607476636\n",
      "635 loss:  0.0024579211603850126 train acc:  1.0 micro:  0.5642523364485982\n",
      "636 loss:  0.002454624744132161 train acc:  1.0 micro:  0.5654205607476636\n",
      "637 loss:  0.0024555472191423178 train acc:  1.0 micro:  0.5654205607476636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638 loss:  0.0024521048180758953 train acc:  1.0 micro:  0.5654205607476636\n",
      "639 loss:  0.0024509842041879892 train acc:  1.0 micro:  0.5665887850467289\n",
      "640 loss:  0.0024510787334293127 train acc:  1.0 micro:  0.5642523364485982\n",
      "641 loss:  0.0024498996790498495 train acc:  1.0 micro:  0.5642523364485982\n",
      "642 loss:  0.0024538678117096424 train acc:  1.0 micro:  0.5642523364485982\n",
      "643 loss:  0.0024499648716300726 train acc:  1.0 micro:  0.5642523364485982\n",
      "644 loss:  0.00244991946965456 train acc:  1.0 micro:  0.5654205607476636\n",
      "645 loss:  0.0024445748422294855 train acc:  1.0 micro:  0.5654205607476636\n",
      "646 loss:  0.0024423617869615555 train acc:  1.0 micro:  0.5642523364485982\n",
      "647 loss:  0.0024446630850434303 train acc:  1.0 micro:  0.5642523364485982\n",
      "648 loss:  0.0024401997216045856 train acc:  1.0 micro:  0.5654205607476636\n",
      "649 loss:  0.002439499367028475 train acc:  1.0 micro:  0.5642523364485982\n",
      "650 loss:  0.0024387193843722343 train acc:  1.0 micro:  0.5654205607476636\n",
      "651 loss:  0.0024378509260714054 train acc:  1.0 micro:  0.5654205607476636\n",
      "652 loss:  0.002440428826957941 train acc:  1.0 micro:  0.5665887850467289\n",
      "653 loss:  0.0024356248322874308 train acc:  1.0 micro:  0.5665887850467289\n",
      "654 loss:  0.002435061614960432 train acc:  1.0 micro:  0.5654205607476636\n",
      "655 loss:  0.0024345493875443935 train acc:  1.0 micro:  0.5665887850467289\n",
      "656 loss:  0.0024327775463461876 train acc:  1.0 micro:  0.5654205607476636\n",
      "657 loss:  0.0024339137598872185 train acc:  1.0 micro:  0.5677570093457944\n",
      "658 loss:  0.0024297384079545736 train acc:  1.0 micro:  0.5677570093457944\n",
      "659 loss:  0.0024286890402436256 train acc:  1.0 micro:  0.5654205607476636\n",
      "660 loss:  0.002429486718028784 train acc:  1.0 micro:  0.5665887850467289\n",
      "661 loss:  0.002427384490147233 train acc:  1.0 micro:  0.5630841121495327\n",
      "662 loss:  0.0024300403892993927 train acc:  1.0 micro:  0.5677570093457944\n",
      "663 loss:  0.002425328129902482 train acc:  1.0 micro:  0.5642523364485982\n",
      "664 loss:  0.0024248913396149874 train acc:  1.0 micro:  0.5665887850467289\n",
      "665 loss:  0.0024220540653914213 train acc:  1.0 micro:  0.5654205607476636\n",
      "666 loss:  0.002420282457023859 train acc:  1.0 micro:  0.5642523364485982\n",
      "667 loss:  0.002421704586595297 train acc:  1.0 micro:  0.5665887850467289\n",
      "668 loss:  0.0024184132926166058 train acc:  1.0 micro:  0.5654205607476636\n",
      "669 loss:  0.0024178901221603155 train acc:  1.0 micro:  0.5665887850467289\n",
      "670 loss:  0.0024164030328392982 train acc:  1.0 micro:  0.5689252336448598\n",
      "671 loss:  0.0024151953402906656 train acc:  1.0 micro:  0.5665887850467289\n",
      "672 loss:  0.002416432835161686 train acc:  1.0 micro:  0.5665887850467289\n",
      "673 loss:  0.002413863781839609 train acc:  1.0 micro:  0.5677570093457944\n",
      "674 loss:  0.0024157315492630005 train acc:  1.0 micro:  0.5677570093457944\n",
      "675 loss:  0.0024100623559206724 train acc:  1.0 micro:  0.5665887850467289\n",
      "676 loss:  0.002409270266070962 train acc:  1.0 micro:  0.5665887850467289\n",
      "677 loss:  0.002410604152828455 train acc:  1.0 micro:  0.5665887850467289\n",
      "678 loss:  0.0024069484788924456 train acc:  1.0 micro:  0.5654205607476636\n",
      "679 loss:  0.0024065368343144655 train acc:  1.0 micro:  0.5654205607476636\n",
      "680 loss:  0.002404420403763652 train acc:  1.0 micro:  0.5642523364485982\n",
      "681 loss:  0.002402673475444317 train acc:  1.0 micro:  0.5654205607476636\n",
      "682 loss:  0.0024028585758060217 train acc:  1.0 micro:  0.5665887850467289\n",
      "683 loss:  0.0024006087332963943 train acc:  1.0 micro:  0.5689252336448598\n",
      "684 loss:  0.0024039598647505045 train acc:  1.0 micro:  0.5665887850467289\n",
      "685 loss:  0.002400161698460579 train acc:  1.0 micro:  0.5642523364485982\n",
      "686 loss:  0.0023993344511836767 train acc:  1.0 micro:  0.5654205607476636\n",
      "687 loss:  0.0023985854350030422 train acc:  1.0 micro:  0.5654205607476636\n",
      "688 loss:  0.0023967509623616934 train acc:  1.0 micro:  0.5654205607476636\n",
      "689 loss:  0.0023972000926733017 train acc:  1.0 micro:  0.5665887850467289\n",
      "690 loss:  0.0023936068173497915 train acc:  1.0 micro:  0.5665887850467289\n",
      "691 loss:  0.002391423797234893 train acc:  1.0 micro:  0.5642523364485982\n",
      "692 loss:  0.002390035195276141 train acc:  1.0 micro:  0.5654205607476636\n",
      "693 loss:  0.002388650318607688 train acc:  1.0 micro:  0.5642523364485982\n",
      "694 loss:  0.0023887003771960735 train acc:  1.0 micro:  0.5642523364485982\n",
      "695 loss:  0.002386650303378701 train acc:  1.0 micro:  0.5665887850467289\n",
      "696 loss:  0.002388664521276951 train acc:  1.0 micro:  0.5654205607476636\n",
      "697 loss:  0.0023837462067604065 train acc:  1.0 micro:  0.5654205607476636\n",
      "698 loss:  0.002382514998316765 train acc:  1.0 micro:  0.5665887850467289\n",
      "699 loss:  0.0023834696039557457 train acc:  1.0 micro:  0.5642523364485982\n",
      "700 loss:  0.0023802814539521933 train acc:  1.0 micro:  0.5654205607476636\n",
      "701 loss:  0.002379484474658966 train acc:  1.0 micro:  0.5642523364485982\n",
      "702 loss:  0.0023789438419044018 train acc:  1.0 micro:  0.5642523364485982\n",
      "703 loss:  0.002376630436629057 train acc:  1.0 micro:  0.5654205607476636\n",
      "704 loss:  0.0023793382570147514 train acc:  1.0 micro:  0.5642523364485982\n",
      "705 loss:  0.0023740283213555813 train acc:  1.0 micro:  0.5654205607476636\n",
      "706 loss:  0.002372557995840907 train acc:  1.0 micro:  0.5654205607476636\n",
      "707 loss:  0.0023729228414595127 train acc:  1.0 micro:  0.5642523364485982\n",
      "708 loss:  0.0023702476173639297 train acc:  1.0 micro:  0.5654205607476636\n",
      "709 loss:  0.0023695153649896383 train acc:  1.0 micro:  0.5654205607476636\n",
      "710 loss:  0.0023683258332312107 train acc:  1.0 micro:  0.5642523364485982\n",
      "711 loss:  0.002366746077314019 train acc:  1.0 micro:  0.5654205607476636\n",
      "712 loss:  0.0023647856432944536 train acc:  1.0 micro:  0.5654205607476636\n",
      "713 loss:  0.0023638249840587378 train acc:  1.0 micro:  0.5654205607476636\n",
      "714 loss:  0.0023617232218384743 train acc:  1.0 micro:  0.5654205607476636\n",
      "715 loss:  0.0023611190263181925 train acc:  1.0 micro:  0.5654205607476636\n",
      "716 loss:  0.002359058940783143 train acc:  1.0 micro:  0.5654205607476636\n",
      "717 loss:  0.002358819590881467 train acc:  1.0 micro:  0.5630841121495327\n",
      "718 loss:  0.0023565473966300488 train acc:  1.0 micro:  0.5642523364485982\n",
      "719 loss:  0.002358271274715662 train acc:  1.0 micro:  0.5642523364485982\n",
      "720 loss:  0.00235473969951272 train acc:  1.0 micro:  0.5642523364485982\n",
      "721 loss:  0.0023537389934062958 train acc:  1.0 micro:  0.5619158878504673\n",
      "722 loss:  0.002351826522499323 train acc:  1.0 micro:  0.5630841121495327\n",
      "723 loss:  0.002350078197196126 train acc:  1.0 micro:  0.5619158878504673\n",
      "724 loss:  0.0023503131233155727 train acc:  1.0 micro:  0.5619158878504673\n",
      "725 loss:  0.002347048604860902 train acc:  1.0 micro:  0.5619158878504673\n",
      "726 loss:  0.0023453086614608765 train acc:  1.0 micro:  0.5619158878504673\n",
      "727 loss:  0.0023443789687007666 train acc:  1.0 micro:  0.5619158878504673\n",
      "728 loss:  0.002342693740502 train acc:  1.0 micro:  0.5665887850467289\n",
      "729 loss:  0.002344034379348159 train acc:  1.0 micro:  0.5607476635514018\n",
      "730 loss:  0.002340981736779213 train acc:  1.0 micro:  0.5630841121495327\n",
      "731 loss:  0.002340590348467231 train acc:  1.0 micro:  0.5630841121495327\n",
      "732 loss:  0.002338266000151634 train acc:  1.0 micro:  0.5630841121495327\n",
      "733 loss:  0.0023361763451248407 train acc:  1.0 micro:  0.5642523364485982\n",
      "734 loss:  0.0023360373452305794 train acc:  1.0 micro:  0.5619158878504673\n",
      "735 loss:  0.00233314442448318 train acc:  1.0 micro:  0.5630841121495327\n",
      "736 loss:  0.002332276664674282 train acc:  1.0 micro:  0.5630841121495327\n",
      "737 loss:  0.0023296780418604612 train acc:  1.0 micro:  0.5630841121495327\n",
      "738 loss:  0.0023286533541977406 train acc:  1.0 micro:  0.5642523364485982\n",
      "739 loss:  0.0023273960687220097 train acc:  1.0 micro:  0.5619158878504673\n",
      "740 loss:  0.0023255781270563602 train acc:  1.0 micro:  0.5654205607476636\n",
      "741 loss:  0.002325502224266529 train acc:  1.0 micro:  0.5630841121495327\n",
      "742 loss:  0.002321988809853792 train acc:  1.0 micro:  0.5654205607476636\n",
      "743 loss:  0.0023220721632242203 train acc:  1.0 micro:  0.5630841121495327\n",
      "744 loss:  0.0023197224363684654 train acc:  1.0 micro:  0.5619158878504673\n",
      "745 loss:  0.0023174749221652746 train acc:  1.0 micro:  0.5630841121495327\n",
      "746 loss:  0.0023173540830612183 train acc:  1.0 micro:  0.5630841121495327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747 loss:  0.0023145992308855057 train acc:  1.0 micro:  0.5619158878504673\n",
      "748 loss:  0.002313013421371579 train acc:  1.0 micro:  0.5642523364485982\n",
      "749 loss:  0.0023122376296669245 train acc:  1.0 micro:  0.5630841121495327\n",
      "750 loss:  0.0023096525110304356 train acc:  1.0 micro:  0.5642523364485982\n",
      "751 loss:  0.002310462761670351 train acc:  1.0 micro:  0.5619158878504673\n",
      "752 loss:  0.0023077428340911865 train acc:  1.0 micro:  0.5630841121495327\n",
      "753 loss:  0.002306740963831544 train acc:  1.0 micro:  0.5619158878504673\n",
      "754 loss:  0.0023042394313961267 train acc:  1.0 micro:  0.5619158878504673\n",
      "755 loss:  0.0023021353408694267 train acc:  1.0 micro:  0.5630841121495327\n",
      "756 loss:  0.0023010920267552137 train acc:  1.0 micro:  0.5619158878504673\n",
      "757 loss:  0.002299490850418806 train acc:  1.0 micro:  0.5642523364485982\n",
      "758 loss:  0.0022989038843661547 train acc:  1.0 micro:  0.5619158878504673\n",
      "759 loss:  0.002296022605150938 train acc:  1.0 micro:  0.5642523364485982\n",
      "760 loss:  0.002295771613717079 train acc:  1.0 micro:  0.5642523364485982\n",
      "761 loss:  0.002292202552780509 train acc:  1.0 micro:  0.5619158878504673\n",
      "762 loss:  0.0022901014890521765 train acc:  1.0 micro:  0.5642523364485982\n",
      "763 loss:  0.0022914032451808453 train acc:  1.0 micro:  0.5619158878504673\n",
      "764 loss:  0.002286560134962201 train acc:  1.0 micro:  0.5642523364485982\n",
      "765 loss:  0.002285673515871167 train acc:  1.0 micro:  0.5642523364485982\n",
      "766 loss:  0.0022853482514619827 train acc:  1.0 micro:  0.5619158878504673\n",
      "767 loss:  0.002282700501382351 train acc:  1.0 micro:  0.5642523364485982\n",
      "768 loss:  0.0022825561463832855 train acc:  1.0 micro:  0.5630841121495327\n",
      "769 loss:  0.0022791994269937277 train acc:  1.0 micro:  0.5619158878504673\n",
      "770 loss:  0.0022762431763112545 train acc:  1.0 micro:  0.5642523364485982\n",
      "771 loss:  0.0022758073173463345 train acc:  1.0 micro:  0.5619158878504673\n",
      "772 loss:  0.0022732263896614313 train acc:  1.0 micro:  0.5630841121495327\n",
      "773 loss:  0.002273040357977152 train acc:  1.0 micro:  0.5630841121495327\n",
      "774 loss:  0.0022719178814440966 train acc:  1.0 micro:  0.5630841121495327\n",
      "775 loss:  0.0022699523251503706 train acc:  1.0 micro:  0.5619158878504673\n",
      "776 loss:  0.002267214935272932 train acc:  1.0 micro:  0.5642523364485982\n",
      "777 loss:  0.002266142051666975 train acc:  1.0 micro:  0.5619158878504673\n",
      "778 loss:  0.0022621669340878725 train acc:  1.0 micro:  0.5630841121495327\n",
      "779 loss:  0.002261816058307886 train acc:  1.0 micro:  0.5619158878504673\n",
      "780 loss:  0.0022600539959967136 train acc:  1.0 micro:  0.5630841121495327\n",
      "781 loss:  0.0022585911210626364 train acc:  1.0 micro:  0.5619158878504673\n",
      "782 loss:  0.002257413463667035 train acc:  1.0 micro:  0.5630841121495327\n",
      "783 loss:  0.0022559582721441984 train acc:  1.0 micro:  0.5630841121495327\n",
      "784 loss:  0.0022539596538990736 train acc:  1.0 micro:  0.5619158878504673\n",
      "785 loss:  0.002254127524793148 train acc:  1.0 micro:  0.5619158878504673\n",
      "786 loss:  0.00225052610039711 train acc:  1.0 micro:  0.5619158878504673\n",
      "787 loss:  0.0022487726528197527 train acc:  1.0 micro:  0.5630841121495327\n",
      "788 loss:  0.0022489740513265133 train acc:  1.0 micro:  0.5619158878504673\n",
      "789 loss:  0.0022452499251812696 train acc:  1.0 micro:  0.5630841121495327\n",
      "790 loss:  0.002245070645585656 train acc:  1.0 micro:  0.5619158878504673\n",
      "791 loss:  0.0022426529321819544 train acc:  1.0 micro:  0.5619158878504673\n",
      "792 loss:  0.0022405479103326797 train acc:  1.0 micro:  0.5619158878504673\n",
      "793 loss:  0.0022398079745471478 train acc:  1.0 micro:  0.5619158878504673\n",
      "794 loss:  0.0022380135487765074 train acc:  1.0 micro:  0.5619158878504673\n",
      "795 loss:  0.0022360794246196747 train acc:  1.0 micro:  0.5619158878504673\n",
      "796 loss:  0.0022356174886226654 train acc:  1.0 micro:  0.5607476635514018\n",
      "797 loss:  0.002232689643278718 train acc:  1.0 micro:  0.5607476635514018\n",
      "798 loss:  0.002231728285551071 train acc:  1.0 micro:  0.5630841121495327\n",
      "799 loss:  0.0022308737970888615 train acc:  1.0 micro:  0.5607476635514018\n",
      "800 loss:  0.002228786237537861 train acc:  1.0 micro:  0.5630841121495327\n",
      "801 loss:  0.00222963304258883 train acc:  1.0 micro:  0.5619158878504673\n",
      "802 loss:  0.002225917764008045 train acc:  1.0 micro:  0.5607476635514018\n",
      "803 loss:  0.0022231254260987043 train acc:  1.0 micro:  0.5630841121495327\n",
      "804 loss:  0.0022229936439543962 train acc:  1.0 micro:  0.5607476635514018\n",
      "805 loss:  0.0022205132991075516 train acc:  1.0 micro:  0.5630841121495327\n",
      "806 loss:  0.002219929825514555 train acc:  1.0 micro:  0.5607476635514018\n",
      "807 loss:  0.0022189533337950706 train acc:  1.0 micro:  0.5619158878504673\n",
      "808 loss:  0.0022169940639287233 train acc:  1.0 micro:  0.5619158878504673\n",
      "809 loss:  0.0022153551690280437 train acc:  1.0 micro:  0.5619158878504673\n",
      "810 loss:  0.0022143060341477394 train acc:  1.0 micro:  0.5630841121495327\n",
      "811 loss:  0.00221342034637928 train acc:  1.0 micro:  0.5607476635514018\n",
      "812 loss:  0.002212418708950281 train acc:  1.0 micro:  0.5630841121495327\n",
      "813 loss:  0.002211654791608453 train acc:  1.0 micro:  0.5607476635514018\n",
      "814 loss:  0.0022098510526120663 train acc:  1.0 micro:  0.5619158878504673\n",
      "815 loss:  0.0022107844706624746 train acc:  1.0 micro:  0.5619158878504673\n",
      "816 loss:  0.0022081874776631594 train acc:  1.0 micro:  0.5607476635514018\n",
      "817 loss:  0.002206510631367564 train acc:  1.0 micro:  0.5642523364485982\n",
      "818 loss:  0.0022067015524953604 train acc:  1.0 micro:  0.5607476635514018\n",
      "819 loss:  0.002203023759648204 train acc:  1.0 micro:  0.5607476635514018\n",
      "820 loss:  0.002201851224526763 train acc:  1.0 micro:  0.5642523364485982\n",
      "821 loss:  0.0022026286460459232 train acc:  1.0 micro:  0.5607476635514018\n",
      "822 loss:  0.0022002607583999634 train acc:  1.0 micro:  0.5630841121495327\n",
      "823 loss:  0.002200415125116706 train acc:  1.0 micro:  0.5619158878504673\n",
      "824 loss:  0.0021992488764226437 train acc:  1.0 micro:  0.5630841121495327\n",
      "825 loss:  0.0021970251109451056 train acc:  1.0 micro:  0.5619158878504673\n",
      "826 loss:  0.002197267021983862 train acc:  1.0 micro:  0.5619158878504673\n",
      "827 loss:  0.002195425797253847 train acc:  1.0 micro:  0.5630841121495327\n",
      "828 loss:  0.0021935380063951015 train acc:  1.0 micro:  0.5630841121495327\n",
      "829 loss:  0.0021937910933047533 train acc:  1.0 micro:  0.5630841121495327\n",
      "830 loss:  0.0021913282107561827 train acc:  1.0 micro:  0.5630841121495327\n",
      "831 loss:  0.0021906630136072636 train acc:  1.0 micro:  0.5630841121495327\n",
      "832 loss:  0.002190574072301388 train acc:  1.0 micro:  0.5642523364485982\n",
      "833 loss:  0.0021884485613554716 train acc:  1.0 micro:  0.5642523364485982\n",
      "834 loss:  0.0021887407638132572 train acc:  1.0 micro:  0.5642523364485982\n",
      "835 loss:  0.002185884863138199 train acc:  1.0 micro:  0.5654205607476636\n",
      "836 loss:  0.0021851081401109695 train acc:  1.0 micro:  0.5654205607476636\n",
      "837 loss:  0.0021853058133274317 train acc:  1.0 micro:  0.5654205607476636\n",
      "838 loss:  0.002183578908443451 train acc:  1.0 micro:  0.5642523364485982\n",
      "839 loss:  0.002182150725275278 train acc:  1.0 micro:  0.5654205607476636\n",
      "840 loss:  0.0021807148586958647 train acc:  1.0 micro:  0.5642523364485982\n",
      "841 loss:  0.0021794126369059086 train acc:  1.0 micro:  0.5654205607476636\n",
      "842 loss:  0.002178348135203123 train acc:  1.0 micro:  0.5665887850467289\n",
      "843 loss:  0.00217729271389544 train acc:  1.0 micro:  0.5654205607476636\n",
      "844 loss:  0.002175308996811509 train acc:  1.0 micro:  0.5665887850467289\n",
      "845 loss:  0.0021763776894658804 train acc:  1.0 micro:  0.5665887850467289\n",
      "846 loss:  0.0021743369288742542 train acc:  1.0 micro:  0.5654205607476636\n",
      "847 loss:  0.0021734328474849463 train acc:  1.0 micro:  0.5665887850467289\n",
      "848 loss:  0.0021733026951551437 train acc:  1.0 micro:  0.5654205607476636\n",
      "849 loss:  0.0021709317807108164 train acc:  1.0 micro:  0.5665887850467289\n",
      "850 loss:  0.002171180909499526 train acc:  1.0 micro:  0.5654205607476636\n",
      "851 loss:  0.00216930010356009 train acc:  1.0 micro:  0.5642523364485982\n",
      "852 loss:  0.0021679128985852003 train acc:  1.0 micro:  0.5654205607476636\n",
      "853 loss:  0.002167739439755678 train acc:  1.0 micro:  0.5654205607476636\n",
      "854 loss:  0.002166043734177947 train acc:  1.0 micro:  0.5654205607476636\n",
      "855 loss:  0.002165735699236393 train acc:  1.0 micro:  0.5654205607476636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856 loss:  0.0021645228844136 train acc:  1.0 micro:  0.5665887850467289\n",
      "857 loss:  0.0021631221752613783 train acc:  1.0 micro:  0.5654205607476636\n",
      "858 loss:  0.0021632693242281675 train acc:  1.0 micro:  0.5665887850467289\n",
      "859 loss:  0.002161406446248293 train acc:  1.0 micro:  0.5665887850467289\n",
      "860 loss:  0.0021609619725495577 train acc:  1.0 micro:  0.5654205607476636\n",
      "861 loss:  0.002159892115741968 train acc:  1.0 micro:  0.5665887850467289\n",
      "862 loss:  0.0021588474046438932 train acc:  1.0 micro:  0.5654205607476636\n",
      "863 loss:  0.0021565305069088936 train acc:  1.0 micro:  0.5654205607476636\n",
      "864 loss:  0.0021554273553192616 train acc:  1.0 micro:  0.5665887850467289\n",
      "865 loss:  0.0021542359609156847 train acc:  1.0 micro:  0.5654205607476636\n",
      "866 loss:  0.0021536608692258596 train acc:  1.0 micro:  0.5677570093457944\n",
      "867 loss:  0.002153020352125168 train acc:  1.0 micro:  0.5642523364485982\n",
      "868 loss:  0.002151221502572298 train acc:  1.0 micro:  0.5654205607476636\n",
      "869 loss:  0.0021514904219657183 train acc:  1.0 micro:  0.5654205607476636\n",
      "870 loss:  0.002149855252355337 train acc:  1.0 micro:  0.5654205607476636\n",
      "871 loss:  0.0021493658423423767 train acc:  1.0 micro:  0.5665887850467289\n",
      "872 loss:  0.002149417996406555 train acc:  1.0 micro:  0.5654205607476636\n",
      "873 loss:  0.0021472752559930086 train acc:  1.0 micro:  0.5654205607476636\n",
      "874 loss:  0.0021472240332514048 train acc:  1.0 micro:  0.5642523364485982\n",
      "875 loss:  0.002145498525351286 train acc:  1.0 micro:  0.5630841121495327\n",
      "876 loss:  0.0021440256386995316 train acc:  1.0 micro:  0.5665887850467289\n",
      "877 loss:  0.0021439380943775177 train acc:  1.0 micro:  0.5630841121495327\n",
      "878 loss:  0.002142293145880103 train acc:  1.0 micro:  0.5665887850467289\n",
      "879 loss:  0.002143094316124916 train acc:  1.0 micro:  0.5642523364485982\n",
      "880 loss:  0.002141763921827078 train acc:  1.0 micro:  0.5642523364485982\n",
      "881 loss:  0.0021397853270173073 train acc:  1.0 micro:  0.5677570093457944\n",
      "882 loss:  0.0021404088474810123 train acc:  1.0 micro:  0.5630841121495327\n",
      "883 loss:  0.002138484036549926 train acc:  1.0 micro:  0.5677570093457944\n",
      "884 loss:  0.002137607429176569 train acc:  1.0 micro:  0.5665887850467289\n",
      "885 loss:  0.002137080766260624 train acc:  1.0 micro:  0.5642523364485982\n",
      "886 loss:  0.0021350854076445103 train acc:  1.0 micro:  0.5677570093457944\n",
      "887 loss:  0.0021350879687815905 train acc:  1.0 micro:  0.5642523364485982\n",
      "888 loss:  0.002133638598024845 train acc:  1.0 micro:  0.5677570093457944\n",
      "889 loss:  0.0021334271878004074 train acc:  1.0 micro:  0.5642523364485982\n",
      "890 loss:  0.002133570611476898 train acc:  1.0 micro:  0.5677570093457944\n",
      "891 loss:  0.0021320153027772903 train acc:  1.0 micro:  0.5654205607476636\n",
      "892 loss:  0.0021312886383384466 train acc:  1.0 micro:  0.5642523364485982\n",
      "893 loss:  0.0021294255275279284 train acc:  1.0 micro:  0.5677570093457944\n",
      "894 loss:  0.0021281868685036898 train acc:  1.0 micro:  0.5630841121495327\n",
      "895 loss:  0.0021274974569678307 train acc:  1.0 micro:  0.5665887850467289\n",
      "896 loss:  0.0021274688187986612 train acc:  1.0 micro:  0.5630841121495327\n",
      "897 loss:  0.0021267712581902742 train acc:  1.0 micro:  0.5665887850467289\n",
      "898 loss:  0.002127360086888075 train acc:  1.0 micro:  0.5665887850467289\n",
      "899 loss:  0.002125627128407359 train acc:  1.0 micro:  0.5642523364485982\n",
      "900 loss:  0.0021251798607409 train acc:  1.0 micro:  0.5665887850467289\n",
      "901 loss:  0.0021247940603643656 train acc:  1.0 micro:  0.5630841121495327\n",
      "902 loss:  0.0021235959138721228 train acc:  1.0 micro:  0.5665887850467289\n",
      "903 loss:  0.002123358892276883 train acc:  1.0 micro:  0.5642523364485982\n",
      "904 loss:  0.0021210969425737858 train acc:  1.0 micro:  0.5642523364485982\n",
      "905 loss:  0.0021205986849963665 train acc:  1.0 micro:  0.5665887850467289\n",
      "906 loss:  0.0021200436167418957 train acc:  1.0 micro:  0.5642523364485982\n",
      "907 loss:  0.0021187609527260065 train acc:  1.0 micro:  0.5665887850467289\n",
      "908 loss:  0.0021188256796449423 train acc:  1.0 micro:  0.5619158878504673\n",
      "909 loss:  0.002116597956046462 train acc:  1.0 micro:  0.5630841121495327\n",
      "910 loss:  0.002116176765412092 train acc:  1.0 micro:  0.5665887850467289\n",
      "911 loss:  0.002115728799253702 train acc:  1.0 micro:  0.5630841121495327\n",
      "912 loss:  0.002115367678925395 train acc:  1.0 micro:  0.5665887850467289\n",
      "913 loss:  0.00211470783688128 train acc:  1.0 micro:  0.5630841121495327\n",
      "914 loss:  0.0021126947831362486 train acc:  1.0 micro:  0.5630841121495327\n",
      "915 loss:  0.0021125359926372766 train acc:  1.0 micro:  0.5642523364485982\n",
      "916 loss:  0.0021120687015354633 train acc:  1.0 micro:  0.5630841121495327\n",
      "917 loss:  0.002111932262778282 train acc:  1.0 micro:  0.5665887850467289\n",
      "918 loss:  0.002112091751769185 train acc:  1.0 micro:  0.5630841121495327\n",
      "919 loss:  0.0021103122271597385 train acc:  1.0 micro:  0.5665887850467289\n",
      "920 loss:  0.002110341563820839 train acc:  1.0 micro:  0.5642523364485982\n",
      "921 loss:  0.0021084793843328953 train acc:  1.0 micro:  0.5630841121495327\n",
      "922 loss:  0.002107767853885889 train acc:  1.0 micro:  0.5654205607476636\n",
      "923 loss:  0.002108056331053376 train acc:  1.0 micro:  0.5619158878504673\n",
      "924 loss:  0.0021062956657260656 train acc:  1.0 micro:  0.5654205607476636\n",
      "925 loss:  0.002105829305946827 train acc:  1.0 micro:  0.5630841121495327\n",
      "926 loss:  0.002105302643030882 train acc:  1.0 micro:  0.5630841121495327\n",
      "927 loss:  0.0021034658420830965 train acc:  1.0 micro:  0.5654205607476636\n",
      "928 loss:  0.002104327315464616 train acc:  1.0 micro:  0.5630841121495327\n",
      "929 loss:  0.002102863509207964 train acc:  1.0 micro:  0.5619158878504673\n",
      "930 loss:  0.0021016665268689394 train acc:  1.0 micro:  0.5654205607476636\n",
      "931 loss:  0.0021026399917900562 train acc:  1.0 micro:  0.5630841121495327\n",
      "932 loss:  0.002100663725286722 train acc:  1.0 micro:  0.5642523364485982\n",
      "933 loss:  0.0021012681536376476 train acc:  1.0 micro:  0.5630841121495327\n",
      "934 loss:  0.0021014525555074215 train acc:  1.0 micro:  0.5619158878504673\n",
      "935 loss:  0.002099471166729927 train acc:  1.0 micro:  0.5654205607476636\n",
      "936 loss:  0.002099564764648676 train acc:  1.0 micro:  0.5630841121495327\n",
      "937 loss:  0.002097625518217683 train acc:  1.0 micro:  0.5619158878504673\n",
      "938 loss:  0.0020964916329830885 train acc:  1.0 micro:  0.5630841121495327\n",
      "939 loss:  0.00209604250267148 train acc:  1.0 micro:  0.5619158878504673\n",
      "940 loss:  0.0020949558820575476 train acc:  1.0 micro:  0.5665887850467289\n",
      "941 loss:  0.0020956876687705517 train acc:  1.0 micro:  0.5630841121495327\n",
      "942 loss:  0.002094530500471592 train acc:  1.0 micro:  0.5642523364485982\n",
      "943 loss:  0.002093753544613719 train acc:  1.0 micro:  0.5630841121495327\n",
      "944 loss:  0.0020929083693772554 train acc:  1.0 micro:  0.5619158878504673\n",
      "945 loss:  0.002091477857902646 train acc:  1.0 micro:  0.5642523364485982\n",
      "946 loss:  0.0020918247755616903 train acc:  1.0 micro:  0.5607476635514018\n",
      "947 loss:  0.0020907530561089516 train acc:  1.0 micro:  0.5630841121495327\n",
      "948 loss:  0.0020904461853206158 train acc:  1.0 micro:  0.5619158878504673\n",
      "949 loss:  0.0020898664370179176 train acc:  1.0 micro:  0.5619158878504673\n",
      "950 loss:  0.0020889907609671354 train acc:  1.0 micro:  0.5630841121495327\n",
      "951 loss:  0.0020889758598059416 train acc:  1.0 micro:  0.5619158878504673\n",
      "952 loss:  0.0020879125222563744 train acc:  1.0 micro:  0.5630841121495327\n",
      "953 loss:  0.002087700180709362 train acc:  1.0 micro:  0.5607476635514018\n",
      "954 loss:  0.0020861232187598944 train acc:  1.0 micro:  0.5607476635514018\n",
      "955 loss:  0.0020859825890511274 train acc:  1.0 micro:  0.5630841121495327\n",
      "956 loss:  0.00208548316732049 train acc:  1.0 micro:  0.5607476635514018\n",
      "957 loss:  0.002084557432681322 train acc:  1.0 micro:  0.5619158878504673\n",
      "958 loss:  0.0020840188954025507 train acc:  1.0 micro:  0.5607476635514018\n",
      "959 loss:  0.0020822242368012667 train acc:  1.0 micro:  0.5607476635514018\n",
      "960 loss:  0.002081738319247961 train acc:  1.0 micro:  0.5607476635514018\n",
      "961 loss:  0.002081166720017791 train acc:  1.0 micro:  0.5607476635514018\n",
      "962 loss:  0.0020808016415685415 train acc:  1.0 micro:  0.5642523364485982\n",
      "963 loss:  0.0020818004850298166 train acc:  1.0 micro:  0.5607476635514018\n",
      "964 loss:  0.0020805569365620613 train acc:  1.0 micro:  0.5607476635514018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965 loss:  0.0020805620588362217 train acc:  1.0 micro:  0.5607476635514018\n",
      "966 loss:  0.0020805865060538054 train acc:  1.0 micro:  0.5607476635514018\n",
      "967 loss:  0.0020793562289327383 train acc:  1.0 micro:  0.5630841121495327\n",
      "968 loss:  0.0020793296862393618 train acc:  1.0 micro:  0.5607476635514018\n",
      "969 loss:  0.002077927114441991 train acc:  1.0 micro:  0.5607476635514018\n",
      "970 loss:  0.002076574368402362 train acc:  1.0 micro:  0.5619158878504673\n",
      "971 loss:  0.002076938049867749 train acc:  1.0 micro:  0.5607476635514018\n",
      "972 loss:  0.0020752884447574615 train acc:  1.0 micro:  0.5619158878504673\n",
      "973 loss:  0.0020754628349095583 train acc:  1.0 micro:  0.5607476635514018\n",
      "974 loss:  0.002075008349493146 train acc:  1.0 micro:  0.5619158878504673\n",
      "975 loss:  0.0020734616555273533 train acc:  1.0 micro:  0.5607476635514018\n",
      "976 loss:  0.002073641400784254 train acc:  1.0 micro:  0.5607476635514018\n",
      "977 loss:  0.0020718532614409924 train acc:  1.0 micro:  0.5630841121495327\n",
      "978 loss:  0.002071494236588478 train acc:  1.0 micro:  0.5607476635514018\n",
      "979 loss:  0.0020719782914966345 train acc:  1.0 micro:  0.5619158878504673\n",
      "980 loss:  0.002071429044008255 train acc:  1.0 micro:  0.5607476635514018\n",
      "981 loss:  0.002070883521810174 train acc:  1.0 micro:  0.5607476635514018\n",
      "982 loss:  0.002071196911856532 train acc:  1.0 micro:  0.5607476635514018\n",
      "983 loss:  0.002069751964882016 train acc:  1.0 micro:  0.5607476635514018\n",
      "984 loss:  0.0020698471926152706 train acc:  1.0 micro:  0.5607476635514018\n",
      "985 loss:  0.0020684238988906145 train acc:  1.0 micro:  0.5619158878504673\n",
      "986 loss:  0.002067719353362918 train acc:  1.0 micro:  0.5607476635514018\n",
      "987 loss:  0.0020675535779446363 train acc:  1.0 micro:  0.5619158878504673\n",
      "988 loss:  0.00206647883169353 train acc:  1.0 micro:  0.5619158878504673\n",
      "989 loss:  0.0020661894232034683 train acc:  1.0 micro:  0.5607476635514018\n",
      "990 loss:  0.002065320499241352 train acc:  1.0 micro:  0.5619158878504673\n",
      "991 loss:  0.0020653079263865948 train acc:  1.0 micro:  0.5607476635514018\n",
      "992 loss:  0.0020642338786274195 train acc:  1.0 micro:  0.5619158878504673\n",
      "993 loss:  0.002064257627353072 train acc:  1.0 micro:  0.5607476635514018\n",
      "994 loss:  0.0020636681001633406 train acc:  1.0 micro:  0.5607476635514018\n",
      "995 loss:  0.0020632727537304163 train acc:  1.0 micro:  0.5607476635514018\n",
      "996 loss:  0.002062019659206271 train acc:  1.0 micro:  0.5619158878504673\n",
      "997 loss:  0.002061913488432765 train acc:  1.0 micro:  0.5619158878504673\n",
      "998 loss:  0.002060825005173683 train acc:  1.0 micro:  0.5630841121495327\n",
      "999 loss:  0.002061299979686737 train acc:  1.0 micro:  0.5619158878504673\n",
      "0.6191588785046729\n"
     ]
    }
   ],
   "source": [
    "main(node_file, link_file, label_file, embedding_file, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de23143",
   "metadata": {},
   "outputs": [],
   "source": [
    " loss:  0.37939170002937317 train acc:  0.5448191013562286 micro:  0.316588785046729\n",
    "184 loss:  0.38017627596855164 train acc:  0.5451542144218879 micro:  0.31542056074766356\n",
    "185 loss:  0.3799729347229004 train acc:  0.5438081095474381 micro:  0.31542056074766356\n",
    "186 loss:  0.3803214132785797 train acc:  0.5458356860530773 micro:  0.308411214953271\n",
    "187 loss:  0.3802218735218048 train acc:  0.5440949772791855 micro:  0.3189252336448598\n",
    "188 loss:  0.38121509552001953 train acc:  0.5456981889684798 micro:  0.3119158878504673\n",
    "189 loss:  0.37983420491218567 train acc:  0.545561294016143 micro:  0.31542056074766356\n",
    "190 loss:  0.3799597918987274 train acc:  0.5433254904561581 micro:  0.33060747663551404\n",
    "191 loss:  0.3819093108177185 train acc:  0.5459046606321211 micro:  0.308411214953271\n",
    "192 loss:  0.3824063837528229 train acc:  0.5441477485509431 micro:  0.3235981308411215\n",
    "193 loss:  0.38051867485046387 train acc:  0.5456296662730763 micro:  0.3142523364485981\n",
    "194 loss:  0.3802700638771057 train acc:  0.545357078934816 micro:  0.3107476635514019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
