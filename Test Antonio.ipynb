{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1155757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, NeighborSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "seed= 10\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df40cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return print('Error')\n",
    "\n",
    "def one_hot_encoding(l):\n",
    "    label_types = torch.unique(l).tolist()\n",
    "    new_labels = []\n",
    "    for i in range(0, len(l)):\n",
    "        tmp = []\n",
    "        for j in range(0, len(label_types)):\n",
    "            tmp.append(0.)\n",
    "        tmp[l[i].item()] = 1.\n",
    "        new_labels.append(tmp)\n",
    "    return torch.tensor(new_labels)     \n",
    "\n",
    "def load_files(node_file_path, links_file_path, label_file_path, embedding_file_path, dataset):\n",
    "    colors = pd.read_csv(node_file_path, sep='\\t', header = None)\n",
    "    colors = colors.dropna(axis=1,how='all')\n",
    "    labels = pd.read_csv(label_file_path, sep='\\t', header = None)\n",
    "    links = pd.read_csv(links_file_path, sep='\\t', header = None)\n",
    "    labels.rename(columns = {0: 'node', 1: 'label'}, inplace = True)\n",
    "    source_nodes_with_labels = labels['node'].values.tolist()\n",
    "    labels = torch.tensor(labels['label'].values)\n",
    "    colors.rename(columns = {0: 'node', 1: 'color'}, inplace = True)\n",
    "    links.rename(columns = {0: 'node_1', 1: 'relation_type', 2: 'node_2'}, inplace = True)\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        embedding = pd.read_csv(embedding_file_path, sep='\\t', header = None)\n",
    "        embedding_number = len(embedding.columns)-2\n",
    "        if embedding_number == 3:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'second embedding', 2: 'first embedding', 3: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 4:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'third embedding', 2: 'second embedding', 3: 'first embedding', 4: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 5:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'fourth embedding', 2: 'third embedding', 3: 'second embedding', 4: 'first_embdding', 5: 'labels'}, inplace = True)\n",
    "        elif embedding_number == 2:\n",
    "            embedding.rename(columns = {0: 'index', 1: 'first embedding', 2: 'labels'}, inplace = True)\n",
    "        return labels, colors, links, embedding\n",
    "    else:\n",
    "        labels_multi  = one_hot_encoding(labels)\n",
    "        # for i in range(0, len(labels)):\n",
    "        #     if labels[i].item() == 0:\n",
    "        #         labels[i] = 1\n",
    "        #     else:\n",
    "        #         labels[i] = 0\n",
    "        return labels, colors, links, source_nodes_with_labels, labels_multi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitting_node_and_labels(lab, feat, src, dataset):\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        node_idx = torch.tensor(feat['node'].values)\n",
    "    else:\n",
    "        node_idx = torch.tensor(src)\n",
    "        \n",
    "    #node_idx = node_idx.cpu().detach().numpy()\n",
    "    #np.random.seed(10)\n",
    "    #np.random.shuffle(node_idx)   \n",
    "    \n",
    "    train_split = int(len(node_idx)*0.8)\n",
    "    test_split = len(node_idx) - train_split\n",
    "    train_idx = node_idx[:train_split]\n",
    "    test_idx = node_idx[-test_split:]\n",
    "\n",
    "    train_y = lab[:train_split]\n",
    "    test_y = lab[-test_split:]\n",
    "    return node_idx, train_idx, train_y, test_idx, test_y\n",
    "\n",
    "\n",
    "def get_node_features(colors):\n",
    "    node_features = pd.get_dummies(colors)\n",
    "    \n",
    "    node_features.drop([\"node\"], axis=1, inplace=True)\n",
    "    \n",
    "    x = node_features.to_numpy().astype(np.float32)\n",
    "    x = np.flip(x, 1).copy()\n",
    "    x = torch.from_numpy(x) \n",
    "    return x\n",
    "\n",
    "def get_edge_index_and_type_no_reverse(links):\n",
    "    edge_index = links.drop(['relation_type'], axis=1)\n",
    "    edge_index = torch.tensor([list(edge_index['node_1'].values), list(edge_index['node_2'].values)])\n",
    "    \n",
    "    edge_type = links['relation_type']\n",
    "    edge_type = torch.tensor(edge_type)\n",
    "    return edge_index, edge_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3dbfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mpgnn_train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_type)\n",
    "    #weight_loss = torch.Tensor([1.,10.])\n",
    "    loss = F.nll_loss(out[data.train_idx].squeeze(-1), data.train_y)# ,weight = weight_loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def mpgnn_test(model, data):\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_type)#.argmax(dim=-1)\n",
    "    loss_test = F.nll_loss(pred[data.test_idx].squeeze(-1), data.test_y)\n",
    "    \n",
    "    train_predictions = torch.argmax(pred[data.train_idx], 1).tolist()\n",
    "    test_predictions = torch.argmax(pred[data.test_idx], 1).tolist()\n",
    "    train_y = data.train_y.tolist()\n",
    "    test_y = data.test_y.tolist()\n",
    "    # train_acc = (train_predictions == train_y).float().mean()\n",
    "    # test_acc = (test_predictions == test_y).float().mean()\n",
    "    f1_train = f1_score(train_predictions, train_y, average='micro')\n",
    "    f1_test_macro = f1_score(test_predictions, test_y, average = 'macro')\n",
    "    f1_test_micro = f1_score(test_predictions, test_y, average = 'micro')\n",
    "    return f1_train, f1_test_micro, f1_test_macro,loss_test,train_predictions,test_predictions\n",
    "\n",
    "\n",
    "def mpgnn_parallel_multiple(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, metapaths):\n",
    "    #metapaths = [[0, 4, 2], [4, 3, 0], [1, 0]] # multi sintetico0\n",
    "    metapaths = [[2,0],[3,1]] #IMDB\n",
    "    #metapaths = [[2,1,0]] # complex = true\n",
    "    mpgnn_model = MPNetm(input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, len(metapaths), metapaths)\n",
    "    print(mpgnn_model)\n",
    "    \n",
    "    mpgnn_optimizer = torch.optim.Adam(mpgnn_model.parameters(), lr=0.1, weight_decay=0.0005) #lr  0.01\n",
    "    best_macro, best_micro = 0., 0.\n",
    "    for epoch in range(1, 300):\n",
    "        loss = mpgnn_train(mpgnn_model, mpgnn_optimizer, data_mpgnn)\n",
    "        if epoch % 20 == 0:\n",
    "            train_acc, f1_test_micro, f1_test_macro,loss_test,ptr,pte = mpgnn_test(mpgnn_model, data_mpgnn)\n",
    "            print(epoch, \"train loss %0.3f\" % loss, \"test loss %0.3f\" % loss_test,\n",
    "                  'train micro: %0.3f'% train_acc, 'test micro: %0.3f'% f1_test_micro)\n",
    "            if f1_test_macro > best_macro:\n",
    "                best_macro = f1_test_micro\n",
    "            if f1_test_micro > best_micro:\n",
    "                best_micro = f1_test_micro\n",
    "    return best_micro,ptr,pte\n",
    "\n",
    "\n",
    "\n",
    "def main(node_file_path, link_file_path, label_file_path, embedding_file_path, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset):\n",
    "    # Obtain true 0|1 labels for each node, feature matrix (1-hot encoding) and links among nodes\n",
    "    if dataset == 'complex' or dataset == 'simple':\n",
    "        sources = []\n",
    "        true_labels, features, edges, embedding = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "    else: \n",
    "        true_labels, features, edges, sources, labels_multi = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "    # Get features' matrix\n",
    "    x = get_node_features(features)\n",
    "    # Get edge_index and types\n",
    "    edge_index, edge_type = get_edge_index_and_type_no_reverse(edges)\n",
    "\n",
    "    # Split data into train and test\n",
    "    node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(true_labels, features, sources, dataset)\n",
    "    #node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(labels_multi, features, sources, dataset)\n",
    "\n",
    "    # Dataset for MPGNN\n",
    "    data_mpgnn = Data()\n",
    "    data_mpgnn.x = x\n",
    "    data_mpgnn.edge_index = edge_index\n",
    "    data_mpgnn.edge_type = edge_type\n",
    "    data_mpgnn.train_idx = train_idx\n",
    "    data_mpgnn.test_idx = test_idx\n",
    "    data_mpgnn.train_y = train_y\n",
    "    data_mpgnn.test_y = test_y\n",
    "    data_mpgnn.num_nodes = len(node_idx)\n",
    "    # Variables\n",
    "    if sources:\n",
    "        source_nodes_mask = sources\n",
    "    else:\n",
    "        source_nodes_mask = []\n",
    "    metapath = []\n",
    "\n",
    "    # Dataset for score function\n",
    "    data = Data()\n",
    "    data.x = x\n",
    "    data.edge_index = edge_index\n",
    "    data.edge_type = edge_type\n",
    "    data.labels = true_labels\n",
    "    data.labels = data.labels.unsqueeze(-1)\n",
    "    data.num_nodes = x.size(0)\n",
    "    data.bags = torch.empty(1)\n",
    "    data.bag_labels = torch.empty(1)\n",
    "\n",
    "    # All possible relations\n",
    "    relations = torch.unique(data.edge_type).tolist()\n",
    "    mp = []\n",
    "    mpgnn_f1_micro,ptr,pte = mpgnn_parallel_multiple(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, mp)\n",
    "    return mpgnn_f1_micro,ptr,pte\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48b6276",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetm(\n",
      "  (layers_list): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): CustomRGCNConv(3066, 32, num_relations=4)\n",
      "      (1): CustomRGCNConv(32, 32, num_relations=4)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): CustomRGCNConv(3066, 32, num_relations=4)\n",
      "      (1): CustomRGCNConv(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "20 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n",
      "40 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n",
      "60 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n",
      "80 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n",
      "100 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n",
      "120 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n",
      "140 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n",
      "160 train loss 1.099 test loss 1.099 train micro: 0.301 test micro: 0.124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m num_rel \u001b[38;5;241m=\u001b[39m tot_rel\n\u001b[1;32m     46\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m---> 48\u001b[0m meta,ptr,pte \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetapath_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mll_output_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 99\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(node_file_path, link_file_path, label_file_path, embedding_file_path, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset)\u001b[0m\n\u001b[1;32m     97\u001b[0m relations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(data\u001b[38;5;241m.\u001b[39medge_type)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     98\u001b[0m mp \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 99\u001b[0m mpgnn_f1_micro,ptr,pte \u001b[38;5;241m=\u001b[39m \u001b[43mmpgnn_parallel_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_mpgnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_rel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mll_output_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mpgnn_f1_micro,ptr,pte\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mmpgnn_parallel_multiple\u001b[0;34m(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, metapaths)\u001b[0m\n\u001b[1;32m     37\u001b[0m best_macro, best_micro \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmpgnn_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmpgnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmpgnn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_mpgnn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m         train_acc, f1_test_micro, f1_test_macro,loss_test,ptr,pte \u001b[38;5;241m=\u001b[39m mpgnn_test(mpgnn_model, data_mpgnn)\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mmpgnn_train\u001b[0;34m(model, optimizer, data)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#weight_loss = torch.Tensor([1.,10.])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[data\u001b[38;5;241m.\u001b[39mtrain_idx]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), data\u001b[38;5;241m.\u001b[39mtrain_y)\u001b[38;5;66;03m# ,weight = weight_loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/france/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Dottorato/Francè/MultirelationalGNN/model.py:241\u001b[0m, in \u001b[0;36mMPNetm.forward\u001b[0;34m(self, x, edge_index, edge_type)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetapaths[i])):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m         h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetapaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m#h = self.dropout1(h)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m         h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers_list[i][layer_index](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetapaths[i][layer_index], h, edge_index, edge_type))\n",
      "File \u001b[0;32m~/anaconda3/envs/france/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Dottorato/Francè/MultirelationalGNN/mp_rgcn_layer.py:248\u001b[0m, in \u001b[0;36mCustomRGCNConv.forward\u001b[0;34m(self, relation, x, edge_index, edge_type)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(tmp, x\u001b[38;5;241m=\u001b[39mx_l, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m    246\u001b[0m                 out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m (h \u001b[38;5;241m@\u001b[39m weight[i])\n\u001b[0;32m--> 248\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m root[x_r] \u001b[38;5;28;01mif\u001b[39;00m x_r\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong \u001b[38;5;28;01melse\u001b[39;00m x_r \u001b[38;5;241m@\u001b[39m root\n",
      "File \u001b[0;32m~/anaconda3/envs/france/lib/python3.10/site-packages/torch/nn/modules/module.py:1256\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_full_backward_hook\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_full_backward_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1258\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "COMPLEX = True\n",
    "COMPLEX = \"synthetic_multi\"\n",
    "COMPLEX = \"IMDB\"\n",
    "\n",
    "metapath_length= 3\n",
    "tot_rel=5\n",
    "\n",
    "if COMPLEX == True:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"complex\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == False:\n",
    "    input_dim = 6\n",
    "    ll_output_dim = 2\n",
    "    dataset = \"simple\"\n",
    "    folder= \"data/\" + dataset + \"/length_m_\" + str(metapath_length) + \"__tot_rel_\" + str(tot_rel) + \"/\"\n",
    "elif COMPLEX == 'IMDB':\n",
    "    tot_rel=4\n",
    "    input_dim = 3066\n",
    "    ll_output_dim = 3\n",
    "    dataset = 'IMDB' ## 5\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'DBLP':\n",
    "    input_dim = 4231\n",
    "    tot_rel=6\n",
    "    ll_output_dim = 4\n",
    "    dataset = 'DBLP' ## 7\n",
    "    folder= \"data/\" + dataset + \"/\"\n",
    "elif COMPLEX == 'synthetic_multi':\n",
    "    input_dim=6\n",
    "    tot_rel=5\n",
    "    ll_output_dim=2\n",
    "    dataset = 'tot_rel_5'\n",
    "    folder=\"data/synthetic_multi/\" + dataset + \"/\"\n",
    "\n",
    "node_file= folder + \"node.dat\"\n",
    "link_file= folder + \"link.dat\"\n",
    "label_file= folder + \"label.dat\"\n",
    "embedding_file = folder + \"embedding.dat\"\n",
    "# Define the filename for saving the variables\n",
    "pickle_filename = folder + \"iteration_variables.pkl\"\n",
    "# mpgnn variables\n",
    "hidden_dim = 32\n",
    "num_rel = tot_rel\n",
    "output_dim = 64\n",
    "\n",
    "meta,ptr,pte = main(node_file, link_file, label_file, embedding_file, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ef24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "280 train loss 0.224 test loss 0.230 train micro: 0.907 test micro: 0.915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5343fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac33c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_mpgnn.train_y),len(data_mpgnn.test_y),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data_mpgnn.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data_mpgnn.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a1ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81cd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73231bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f6808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e2abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6b0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dabc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34fdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_file_path, link_file_path, label_file_path, embedding_file_path, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset = node_file, link_file, label_file, embedding_file, metapath_length, pickle_filename, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75768400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain true 0|1 labels for each node, feature matrix (1-hot encoding) and links among nodes\n",
    "if dataset == 'complex' or dataset == 'simple':\n",
    "    sources = []\n",
    "    true_labels, features, edges, embedding = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "else: \n",
    "    true_labels, features, edges, sources, labels_multi = load_files(node_file_path, link_file_path, label_file_path, embedding_file_path, dataset)\n",
    "# Get features' matrix\n",
    "x = get_node_features(features)\n",
    "# Get edge_index and types\n",
    "edge_index, edge_type = get_edge_index_and_type_no_reverse(edges)\n",
    "\n",
    "# Split data into train and test\n",
    "node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(true_labels, features, sources, dataset)\n",
    "#node_idx, train_idx, train_y, test_idx, test_y = splitting_node_and_labels(labels_multi, features, sources, dataset)\n",
    "\n",
    "# Dataset for MPGNN\n",
    "data_mpgnn = Data()\n",
    "data_mpgnn.x = x\n",
    "data_mpgnn.edge_index = edge_index\n",
    "data_mpgnn.edge_type = edge_type\n",
    "data_mpgnn.train_idx = train_idx\n",
    "data_mpgnn.test_idx = test_idx\n",
    "data_mpgnn.train_y = train_y\n",
    "data_mpgnn.test_y = test_y\n",
    "data_mpgnn.num_nodes = len(node_idx)\n",
    "# Variables\n",
    "if sources:\n",
    "    source_nodes_mask = sources\n",
    "else:\n",
    "    source_nodes_mask = []\n",
    "metapath = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = []\n",
    "\n",
    "#metapaths = [[2, 4, 0], [0, 3, 4], [0, 1]]\n",
    "metapaths = [[2,0],[3,1]] #IMDB\n",
    "\n",
    "\n",
    "data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, mp = data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, metapaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e7938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mpgnn_model = MPNetm(input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, len(metapaths), metapaths)\n",
    "print(mpgnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e4127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpgnn_parallel_multiple(data_mpgnn, input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, metapaths):\n",
    "    metapaths = [[2, 4, 0], [0, 3, 4], [0, 1]] # multi sintetico\n",
    "    #metapaths = [[2,0],[3,1]] #IMDB\n",
    "    #metapaths = [[2,1,0]] # complex = true\n",
    "    mpgnn_model = MPNetm(input_dim, hidden_dim, num_rel, output_dim, ll_output_dim, len(metapaths), metapaths)\n",
    "    print(mpgnn_model)\n",
    "    \n",
    "    best_macro, best_micro = 0., 0.\n",
    "    for epoch in range(1, 1000):\n",
    "        loss = mpgnn_train(mpgnn_model, mpgnn_optimizer, data_mpgnn)\n",
    "        if epoch % 20 == 0:\n",
    "            train_acc, f1_test_micro, f1_test_macro,loss_test = mpgnn_test(mpgnn_model, data_mpgnn)\n",
    "            print(epoch, \"train loss %0.3f\" % loss, \"test loss %0.3f\" % loss_test,'train micro: ', train_acc, 'test micro: ', f1_test_micro)\n",
    "            if f1_test_macro > best_macro:\n",
    "                best_macro = f1_test_micro\n",
    "            if f1_test_micro > best_micro:\n",
    "                best_micro = f1_test_micro\n",
    "    return best_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpgnn_optimizer = torch.optim.Adam(mpgnn_model.parameters(), lr=0.1, weight_decay=0.0005) #lr  0.01\n",
    "\n",
    "mpgnn_model.train()\n",
    "mpgnn_optimizer.zero_grad()\n",
    "weight_loss = torch.tensor([1., 100.])\n",
    "out = mpgnn_model(data_mpgnn.x, data_mpgnn.edge_index, data_mpgnn.edge_type)\n",
    "loss = F.nll_loss(out[data_mpgnn.train_idx].squeeze(-1), data_mpgnn.train_y)\n",
    "loss.backward()\n",
    "mpgnn_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metapaths = [[2,0],[3,1]] #IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072633f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = mpgnn_model.layers_list[0][0](2,data_mpgnn.x,data_mpgnn.edge_index,data_mpgnn.edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpgnn_model.layers_list[0][1](0,h,data_mpgnn.edge_index,data_mpgnn.edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8488a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809df7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(0, len(self.metapaths)):\n",
    "    for layer_index in range(0, len(self.metapaths[i])):\n",
    "        if layer_index == 0:\n",
    "            h = F.relu(self.layers_list[i][layer_index](self.metapaths[i][layer_index], x, edge_index, edge_type))\n",
    "            #h = self.dropout1(h)\n",
    "        else:\n",
    "            h = F.relu(self.layers_list[i][layer_index](self.metapaths[i][layer_index], h, edge_index, edge_type))\n",
    "            #h = self.dropout2(h)\n",
    "    embeddings.append(h)\n",
    "print(embeddings)\n",
    "concatenated_embedding = torch.cat(embeddings, dim=1)\n",
    "h = F.relu(self.fc1(concatenated_embedding))\n",
    "\n",
    "h = F.relu(self.fc2(h))\n",
    "h = self.log_softmax(h)\n",
    "return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9f157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9676553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_macro, best_micro = 0., 0.\n",
    "for epoch in range(1, 1000):\n",
    "    loss = mpgnn_train(mpgnn_model, mpgnn_optimizer, data_mpgnn)\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc, f1_test_micro, f1_test_macro,loss_test = mpgnn_test(mpgnn_model, data_mpgnn)\n",
    "        print(epoch, \"train loss %0.3f\" % loss, \"test loss %0.3f\" % loss_test,'train micro: ', train_acc, 'test micro: ', f1_test_micro)\n",
    "        if f1_test_macro > best_macro:\n",
    "            best_macro = f1_test_micro\n",
    "        if f1_test_micro > best_micro:\n",
    "            best_micro = f1_test_micro\n",
    "return best_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09971c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
